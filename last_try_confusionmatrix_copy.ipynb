{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP85lWrSwksaQTX/jjXKFaa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Google Drive'ı bağlama\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWUeLDkm5nok",
        "outputId": "98e3255c-88b2-4e2e-9aeb-164ee6b2a153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "# 2. Dosyayı yükle (seçmeli)\n",
        "uploaded = files.upload()   # colabe dosyaa yükleme\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "U5X9LY_K8W6v",
        "outputId": "7c955ee3-3751-4cc3-9e53-25e8bbee88e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e4a3293d-c0ce-47e8-b7d3-0ea74eba69e7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e4a3293d-c0ce-47e8-b7d3-0ea74eba69e7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving tekstil3test.zip to tekstil3test.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#colabde yüklü dosyayı başka yere taşıma\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "# 3. Her yüklenen dosyayı Drive'daki aynı adlı klasöre taşı\n",
        "for filename in uploaded.keys():\n",
        "    # Drive'daki hedef klasör yolu (örnek: MyDrive/tekstil3test)\n",
        "    drive_folder = f\"#your file path{filename.split('.')[0]}\"  # klasör adı: dosya adı (uzantısız)\n",
        "\n",
        "    # Eğer klasör yoksa oluştur\n",
        "    if not os.path.exists(drive_folder):\n",
        "        os.makedirs(drive_folder)\n",
        "\n",
        "    # Yüklenen dosyanın geçici yolu\n",
        "    src_path = f\"/content/{filename}\"\n",
        "\n",
        "    # Drive'daki hedef dosya yolu\n",
        "    dest_path = os.path.join(drive_folder, filename)\n",
        "\n",
        "    # Dosyayı taşı\n",
        "    shutil.move(src_path, dest_path)\n",
        "\n",
        "    print(f\"{filename} dosyası Drive'daki {drive_folder} klasörüne taşındı.\")"
      ],
      "metadata": {
        "id": "KrBAok1h8l5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# colabde yüklü dosyayı bir klsörde açma zipli olan dosyayı\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "drive_folder = \"#your file path\"  # açmak istediğin klasör yolu\n",
        "zip_path = os.path.join(drive_folder, \"tekstil3test.zip\")  # zip dosyanın tam adı\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(drive_folder)\n",
        "\n",
        "print(\"Zip dosyası başarıyla açıldı.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKcJ1frs83yU",
        "outputId": "7b5f9853-b05a-44ae-f195-de7070dece88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip dosyası başarıyla açıldı.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# faster r cnn ile eğtitirken oluşan bir json annotations dosyasınının json da categories ksımını değiştirme\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Dosya yolu (train annotations dosyası)\n",
        "train_ann_path = \"#your file path/train/_annotations.coco.json\"\n",
        "\n",
        "# Eğitimdeki sınıflar (supercategory olmadan)\n",
        "categories_new = [\n",
        "    {\"id\": 0, \"name\": \"defect\"},\n",
        "    {\"id\": 1, \"name\": \"1- patlak\"},\n",
        "    {\"id\": 2, \"name\": \"2- igne_kirigi\"},\n",
        "    {\"id\": 3, \"name\": \"3- jut\"},\n",
        "    {\"id\": 4, \"name\": \"5- likra_kacigi\"},\n",
        "    {\"id\": 5, \"name\": \"6- yag_lekesi\"},\n",
        "    {\"id\": 6, \"name\": \"8- May cizgisi\"},\n",
        "    {\"id\": 7, \"name\": \"fsa\"}   # fsa default olarak eklenen class adı\n",
        "]\n",
        "\n",
        "# JSON dosyasını aç\n",
        "with open(train_ann_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Mevcut categories'den id:name map\n",
        "old_cat_id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in data[\"categories\"]}\n",
        "\n",
        "# Yeni categories name:id map (kolay eşleme için)\n",
        "new_cat_name_to_id = {cat[\"name\"]: cat[\"id\"] for cat in categories_new}\n",
        "\n",
        "# Annotationlardaki category_id'leri yeni id'lere eşle\n",
        "for ann in data[\"annotations\"]:\n",
        "    old_id = ann[\"category_id\"]\n",
        "    old_name = old_cat_id_to_name.get(old_id)\n",
        "    if old_name is None:\n",
        "        print(f\"Uyarı: Eski ID {old_id} için isim bulunamadı!\")\n",
        "        continue\n",
        "    new_id = new_cat_name_to_id.get(old_name)\n",
        "    if new_id is None:\n",
        "        print(f\"Uyarı: Yeni kategori listesinde '{old_name}' yok!\")\n",
        "        continue\n",
        "    ann[\"category_id\"] = new_id\n",
        "\n",
        "# categories kısmını tamamen güncelle, supercategory olmadan\n",
        "data[\"categories\"] = categories_new\n",
        "\n",
        "# Dosyayı kaydet (istersen farklı isimde)\n",
        "fixed_train_ann_path = \"#your file path/train/_annotations_fixed.coco.json\"\n",
        "with open(fixed_train_ann_path, \"w\") as f:\n",
        "    json.dump(data, f, indent=4)\n",
        "\n",
        "print(f\"Train annotation dosyası güncellendi ve {fixed_train_ann_path} olarak kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "d3X449IZ9vBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Dosya yolu (örnek: train klasöründeki dosya)\n",
        "json_path = \"#your file path/train/ikinci.coco.json\"\n",
        "\n",
        "# Dosyayı oku\n",
        "with open(json_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Kategorileri yazdır\n",
        "print(\"📦 Categories:\")\n",
        "for cat in data.get(\"categories\", []):\n",
        "    print(f\"🆔 {cat['id']} - {cat['name']}\")\n"
      ],
      "metadata": {
        "id": "A0NDOJVf98PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugf7RzVPxx_x"
      },
      "outputs": [],
      "source": [
        "#FASTER R CNN İÇİN PAKET VE DOSYA KURULUMU\n",
        "\n",
        "\n",
        "# Mevcut Colab ortamındaki PyTorch ve diğer paketlerin bağımlılık çakışmasını gidermek için:\n",
        "# 1. Önce, Colab'in önceden yüklediği ve çakışmaya neden olabilecek tüm paketleri kaldırıyoruz.\n",
        "!pip uninstall -y detectron2 fvcore torch torchvision torchaudio fastai\n",
        "# 2. Ardından, Colab'deki mevcut CUDA sürümüyle uyumlu PyTorch sürümünü kuruyoruz.\n",
        "!pip install torch torchvision torchaudio\n",
        "# 3. Son olarak, Detectron2'yi doğrudan GitHub deposundan kuruyoruz.\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FASTER R CNN İÇİN PAKET VE DOSYA KURULUMU\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n"
      ],
      "metadata": {
        "id": "CkLKXJ3S6AKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YENİ ANNATATİONS OLUŞTURMA\n",
        "!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from PIL import Image\n",
        "\n",
        "# --- Dosya Yolları ve Sınıf İsimleri ---\n",
        "# Kendi dosya yollarınızı burada güncelleyin\n",
        "model_path = \"#your file path/model_final.pth\"   # hazırlanan modelin drive kaydı\n",
        "config_path = \"#your file path/config.yaml\"\n",
        "data_dir = \"/#your file path/train/\"\n",
        "test_annotations_path = os.path.join(data_dir, \"_annotations_fixed.coco.json\")\n",
        "output_annotations_path = os.path.join(data_dir, \"_annotations_fixed2.coco.json\")\n",
        "\n",
        "# Kendi sınıf listenizi buraya girin (YAML dosyasından da çekilebilir, ancak bu daha kolay)\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# --- 1. Detectron2 Dataset'i Kaydetme ---\n",
        "try:\n",
        "    register_coco_instances(\"my_dataset_test\", {}, test_annotations_path, data_dir)\n",
        "    MetadataCatalog.get(\"my_dataset_test\").set(thing_classes=class_names)\n",
        "    print(\"Dataset registered successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error registering dataset: {e}\")\n",
        "\n",
        "# --- 2. Modeli ve Konfigürasyonu Yükleme ---\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "\n",
        "cfg.MODEL.WEIGHTS = model_path\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2  # Güvenilirlik eşiği (trash_score) 0.3\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"Model loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# --- 3. Tahminleri Alıp Yeni Annotation Dosyası Oluşturma ---\n",
        "# Test veri setini Detectron2 formatında yükleyin\n",
        "test_dicts = DatasetCatalog.get(\"my_dataset_test\")\n",
        "\n",
        "new_coco_annotations = {\n",
        "    \"info\": {\"description\": \"Annotations generated by Detectron2 model with score threshold 0.3\"},\n",
        "    \"licenses\": [],\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": []\n",
        "}\n",
        "\n",
        "# Kategori listesini COCO formatına uygun şekilde oluşturun\n",
        "for idx, name in enumerate(class_names):\n",
        "    new_coco_annotations['categories'].append({\"id\": idx + 1, \"name\": name, \"supercategory\": \"none\"})\n",
        "\n",
        "annotation_id_counter = 1\n",
        "\n",
        "# Tahminleri al ve yeni annotation'ları oluştur\n",
        "for d in test_dicts:\n",
        "    img_path = d[\"file_name\"]\n",
        "    img_id = d[\"image_id\"]\n",
        "    height, width = d[\"height\"], d[\"width\"]\n",
        "\n",
        "    # Görüntü dosyası yolunu tam olarak oluştur\n",
        "    full_img_path = os.path.join(data_dir, img_path)\n",
        "\n",
        "    if not os.path.exists(full_img_path):\n",
        "        print(f\"Warning: Image file not found at {full_img_path}. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing image: {img_path}\")\n",
        "\n",
        "    try:\n",
        "        # Predictor ile tahmin yapın\n",
        "        im = Image.open(full_img_path).convert(\"RGB\")\n",
        "        outputs = predictor(im)\n",
        "\n",
        "        instances = outputs[\"instances\"].to(\"cpu\")\n",
        "        boxes = instances.pred_boxes.tensor.numpy()\n",
        "        scores = instances.scores.numpy()\n",
        "        labels = instances.pred_classes.numpy()\n",
        "\n",
        "        # Resim bilgilerini ekleyin\n",
        "        new_coco_annotations[\"images\"].append({\n",
        "            \"id\": img_id,\n",
        "            \"file_name\": os.path.basename(full_img_path),\n",
        "            \"width\": width,\n",
        "            \"height\": height\n",
        "        })\n",
        "\n",
        "        # Tahminleri COCO formatına dönüştürün\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            bbox = [float(x_min), float(y_min), float(x_max - x_min), float(y_max - y_min)]\n",
        "\n",
        "            # Detectron2'nin 0'dan başlayan label'ına 1 ekleyerek COCO uyumlu category_id oluştur\n",
        "            coco_category_id = int(label) + 1\n",
        "\n",
        "            annotation = {\n",
        "                \"id\": annotation_id_counter,\n",
        "                \"image_id\": img_id,\n",
        "                \"category_id\": coco_category_id,\n",
        "                \"bbox\": bbox,\n",
        "                \"area\": float((x_max - x_min) * (y_max - y_min)),\n",
        "                \"iscrowd\": 0,\n",
        "                \"score\": float(score)\n",
        "            }\n",
        "            new_coco_annotations[\"annotations\"].append(annotation)\n",
        "            annotation_id_counter += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "# --- 4. Yeni Annotation Dosyasını Kaydetme ---\n",
        "with open(output_annotations_path, 'w') as f:\n",
        "    json.dump(new_coco_annotations, f, indent=4)\n",
        "\n",
        "print(f\"\\nNew COCO annotations file created at: {output_annotations_path}\")"
      ],
      "metadata": {
        "id": "0WLiymN16UvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.model_zoo import get_config_file, get_checkpoint_url\n",
        "import os\n",
        "\n",
        "setup_logger()\n",
        "\n",
        "def register_my_dataset():\n",
        "    from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "    register_coco_instances(\n",
        "        \"tekstil_train_tek3\", {},\n",
        "        \"#your file path/train/_annotations.coco.json\",\n",
        "        \"#your file path/train\"\n",
        "    )\n",
        "    register_coco_instances(\n",
        "        \"tekstil_val_tek3\", {},\n",
        "        \"#your file path/valid/_annotations.coco.json\",\n",
        "        \"/#your file path/valid\"\n",
        "    )\n",
        "\n",
        "register_my_dataset()\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"tekstil_train_tek3\",)\n",
        "cfg.DATASETS.TEST = (\"tekstil_val_tek3\",)  # veya () boş olabilir\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # önceden eğitilmiş ağırlıklar\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 5000  # ihtiyacına göre arttır\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 8  # senin sınıf sayın\n",
        "\n",
        "metadata = MetadataCatalog.get(\"tekstil_train\")\n",
        "metadata.thing_classes = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "model_path = os.path.join(cfg.OUTPUT_DIR, \"model3_final.pth\")\n",
        "trainer.checkpointer.save(\"model3_final\")\n",
        "print(f\"Model {model_path} olarak kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "4E_61vYQ-Y8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.model_zoo import get_config_file, get_checkpoint_url\n",
        "import os\n",
        "\n",
        "# Logger ayarla\n",
        "setup_logger()\n",
        "\n",
        "# 1. Datasetleri register et\n",
        "def register_datasets():\n",
        "    register_coco_instances(\n",
        "        \"tekstil_train_v3_teks33\", {},\n",
        "        \"/#your file path/train/_annotations.coco.json\",\n",
        "        \"/#your file path/train\"\n",
        "    )\n",
        "    register_coco_instances(\n",
        "        \"tekstil_val_v3_teks33\", {},\n",
        "        \"/#your file path/valid/_annotations.coco.json\",\n",
        "        \"/#your file path/valid\"\n",
        "    )\n",
        "\n",
        "register_datasets()\n",
        "\n",
        "# 2. Sınıf isimleri (metadata için)\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# Metadata'ya sınıfları ekle\n",
        "MetadataCatalog.get(\"tekstil_train_v3_teks33\").thing_classes = class_names\n",
        "MetadataCatalog.get(\"tekstil_val_v3_teks33\").thing_classes = class_names\n",
        "\n",
        "# 3. Config ayarla\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"tekstil_train_v3_teks33\",)\n",
        "cfg.DATASETS.TEST = (\"tekstil_val_v3_teks33\",)  # ya da ()\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # önceden eğitilmiş ağırlıklar\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 5000  # ihtiyaca göre artırabilirsin\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "\n",
        "# Çıktı dosyalarının kaydedileceği klasör\n",
        "cfg.OUTPUT_DIR = \"/#your file path/model_teks3_v2\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 4. Trainer oluştur ve eğitimi başlat\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "# 5. Eğitim sonunda modeli kaydet\n",
        "model_path = os.path.join(cfg.OUTPUT_DIR, \"model3_final.pth\")\n",
        "trainer.checkpointer.save(\"model3_final\")\n",
        "print(f\"Model {model_path} olarak kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "Hq8fWVp0EDBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.model_zoo import get_config_file, get_checkpoint_url\n",
        "import os\n",
        "\n",
        "# Logger ayarla\n",
        "setup_logger()\n",
        "\n",
        "# 1. Datasetleri register et\n",
        "def register_datasets():\n",
        "    register_coco_instances(\n",
        "        \"tekstil_train_v5\", {},\n",
        "        \"/#your file path/train/_annotations.coco.json\",\n",
        "        \"/#your file path/train\"\n",
        "    )\n",
        "    register_coco_instances(\n",
        "        \"tekstil_val_v5\", {},\n",
        "        \"/#your file path/valid/_annotations.coco.json\",\n",
        "        \"/#your file path/valid\"\n",
        "    )\n",
        "\n",
        "register_datasets()\n",
        "\n",
        "# 2. Sınıf isimleri\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# Metadata'ya sınıfları ekle\n",
        "MetadataCatalog.get(\"tekstil_train_v5\").thing_classes = class_names\n",
        "MetadataCatalog.get(\"tekstil_val_v5\").thing_classes = class_names\n",
        "\n",
        "# 3. Config ayarla\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"tekstil_train_v5\",)\n",
        "cfg.DATASETS.TEST = (\"tekstil_val_v5\",)  # istersen boş bırakabilirsin: ()\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # önceden eğitilmiş ağırlıklar\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 8000  # ihtiyacına göre artırabilirsin\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "\n",
        "# 4. Çıktı klasörü\n",
        "cfg.OUTPUT_DIR = \"#your file path/model_teks_v5\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 5. Trainer oluştur ve eğitimi başlat\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "# 6. Model ağırlıklarını kaydet\n",
        "model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "trainer.checkpointer.save(\"model_final\")\n",
        "print(f\"Model {model_path} olarak kaydedildi.\")\n",
        "\n",
        "# 7. Config dosyasını yaml olarak kaydet\n",
        "config_path = os.path.join(cfg.OUTPUT_DIR, \"config.yaml\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(cfg.dump())\n",
        "print(f\"Config dosyası {config_path} olarak kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "D_q6JtqOM6I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KESİN  ANALİZ\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# 1. Sınıf isimleri\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# 2. Metadata ayarı\n",
        "dataset_name = \"tekstil_val_v5\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# 3. Config ve model yükle\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"/#your file path/model_teks_v5/config.yaml\")\n",
        "cfg.MODEL.WEIGHTS = \"/#your file path/model_teks_v5/model_final.pth\"\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# 4. Test klasöründeki görselleri analiz et\n",
        "test_folder = \"/#your file path/tekstil3test/train\"\n",
        "image_files = [f for f in os.listdir(test_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "for img_name in image_files:\n",
        "    img_path = os.path.join(test_folder, img_name)\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"Yüklenemedi: {img_name}\")\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "    # Görselleştir (renk kontrastını değiştirmeden)\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    img_result = out.get_image()[:, :, ::-1]  # BGR formatında\n",
        "\n",
        "    # Göster\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(cv2.cvtColor(img_result, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(img_name)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "GqpsPOsIHqgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# === 1. Paths ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "test_images_dir = \"/#your file path/train\"\n",
        "annotations_path = os.path.join(test_images_dir, \"_annotations_fixed2.coco.json\")\n",
        "output_dir = \"/#your file path/test_outputs_confusion_final2\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Class names ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ===\n",
        "dataset_name = \"tekstil_val_v5\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Config setup ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.25\n",
        "cfg.MODEL.DEVICE = \"cuda\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# === 5. Load annotations ===\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "\n",
        "filename_to_id = {img[\"file_name\"]: img[\"id\"] for img in coco_ann[\"images\"]}\n",
        "gt_map = {}\n",
        "for ann in coco_ann[\"annotations\"]:\n",
        "    image_id = ann[\"image_id\"]\n",
        "    cat_id = ann[\"category_id\"]\n",
        "    gt_map.setdefault(image_id, []).append(cat_id)\n",
        "\n",
        "# === 6. Prediction & comparison ===\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for file in tqdm(os.listdir(test_images_dir)):\n",
        "    if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        continue\n",
        "\n",
        "    image_path = os.path.join(test_images_dir, file)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(file)\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for gt_class in gt_classes:\n",
        "        if pred_classes:\n",
        "            y_true.append(gt_class)\n",
        "            y_pred.append(pred_classes[0])\n",
        "        else:\n",
        "            y_true.append(gt_class)\n",
        "            y_pred.append(-1)  # no prediction\n",
        "\n",
        "    # Save annotated image\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, file)\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 7. Confusion Matrix ===\n",
        "labels = list(range(len(class_names))) + [-1]\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# === 8. Classification Report ===\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX3MJb-VSkJM",
        "outputId": "961b56db-80a1-44d0-fdb4-70f043c9f73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 129/129 [02:46<00:00,  1.29s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# === 1. Paths ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "test_images_dir = \"/#your file path/train\"\n",
        "# Orijinal ground truth (doğru etiket) dosyasını kullanın\n",
        "annotations_path = os.path.join(test_images_dir, \"_annotations_fixed.coco.json\")\n",
        "output_dir = \"/#your file path/test_outputs_confusion_final4\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Class names ===\n",
        "# class_names listesi, Detectron2'nin label_id'sine (0, 1, 2...) karşılık gelir\n",
        "# Fakat COCO'da label_id'ler genellikle 1'den başlar.\n",
        "# Kodun içindeki logic bu durumu dikkate alacaktır.\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ===\n",
        "dataset_name = \"tekstil_val_v5\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Config setup ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3\n",
        "cfg.MODEL.DEVICE = \"cuda\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# === 5. Load annotations ===\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "\n",
        "filename_to_id = {img[\"file_name\"]: img[\"id\"] for img in coco_ann[\"images\"]}\n",
        "gt_map = {}\n",
        "for ann in coco_ann[\"annotations\"]:\n",
        "    image_id = ann[\"image_id\"]\n",
        "    # COCO'daki etiketler 1'den başladığı için, 0'dan başlayan model etiketleriyle\n",
        "    # eşleştirmek için bir harita oluşturulabilir veya 1 çıkarılabilir\n",
        "    cat_id = ann[\"category_id\"]\n",
        "    gt_map.setdefault(image_id, []).append(cat_id)\n",
        "\n",
        "# === 6. Prediction & comparison ===\n",
        "y_true, y_pred = [], []\n",
        "image_files = [f for f in os.listdir(test_images_dir) if f in filename_to_id]\n",
        "\n",
        "for file in tqdm(image_files):\n",
        "    image_path = os.path.join(test_images_dir, file)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "    image_id = filename_to_id[file]\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    # Tahminlerin ve ground truth'ların eşleşmesi için basitleştirilmiş bir döngü\n",
        "    # Her bir ground truth için bir tahmin eşleştirilmeye çalışılıyor\n",
        "    for i, gt_class_id in enumerate(gt_classes):\n",
        "        # Ground truth etiketini y_true'ya ekle\n",
        "        y_true.append(gt_class_id)\n",
        "\n",
        "        # Eğer bir tahmin varsa, ilk tahmin edilen sınıfı kullan\n",
        "        if i < len(pred_classes):\n",
        "            # Model 0'dan başlayan etiketler ürettiği için +1 ekliyoruz\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            # Tahmin yoksa, \"None\" olarak işaretle\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    # Eğer ground truth'tan daha fazla tahmin varsa, kalan tahminleri de ekle\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2) # Ekstra tahmin için özel bir etiket\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    # Save annotated image\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, file)\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 7. Confusion Matrix ===\n",
        "# Etiketleri, 1'den başlayan sınıf ID'leri ve -1 (None) ile oluştur\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "# Sınıflandırma raporu ve karmaşıklık matrisi için gerekli etiketleri hazırlayın\n",
        "# Eğer y_true veya y_pred'de -2 etiketi varsa, bunu da dikkate alın\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# === 8. Classification Report ===\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "print(f\"Confusion Matrix and Classification Report saved to {output_dir}\")\n",
        "print(\"Script finished successfully.\")"
      ],
      "metadata": {
        "id": "iG0MEYeXKYMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import load_coco_json\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# İki ayrı test veri yolu ve annotation dosyası\n",
        "test_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(test_dir_1, \"_annotations_fixed2.coco.json\")\n",
        "\n",
        "test_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(test_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "output_dir = \"/#your file path/combined_test_outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Sınıf İsimleri ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ve Config ===\n",
        "dataset_name = \"tekstil_combined_test\"\n",
        "# Toplam veri seti için tek bir meta veri nesnesi oluşturun\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# Modeli ve config'i yükleyin\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"Model loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 4. Birleşik Ground Truth Verilerini Yükleme ===\n",
        "# Her iki veri setinin etiketlerini ve görsel yollarını birleştirin\n",
        "combined_dicts = []\n",
        "combined_dicts.extend(load_coco_json(annotations_path_1, test_dir_1))\n",
        "combined_dicts.extend(load_coco_json(annotations_path_2, test_dir_2))\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "y_true, y_pred = [], []\n",
        "image_files_with_paths = []\n",
        "\n",
        "for d in combined_dicts:\n",
        "    # `d[\"file_name\"]` zaten tam yolu içeriyor\n",
        "    image_files_with_paths.append(d[\"file_name\"])\n",
        "\n",
        "for image_path in tqdm(image_files_with_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    # Ground truth sınıflarını yüklenen verilerden alın\n",
        "    # COCO'da kategori ID'leri 1'den başlar, bu yüzden 1'den başlayan ID'ler y_true'ya eklenir\n",
        "    gt_classes = [ann['category_id'] for ann in d.get('annotations', []) if 'category_id' in ann]\n",
        "\n",
        "    # Tahminlerin ve ground truth'ların eşleşmesi için basit döngü\n",
        "    # Her bir ground truth için bir tahmin eşleştirilir.\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "\n",
        "        if i < len(pred_classes):\n",
        "            # Model 0'dan başlayan etiketler ürettiği için +1 ekliyoruz (COCO ID'si için)\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1) # Tahmin yoksa, \"None\" olarak işaretle\n",
        "\n",
        "    # Fazla tahminleri (False Positives) ekle\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2) # Ekstra tahmin için özel bir etiket\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    # Görseli kaydet\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    # Çıktı dosya adı\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Karmaşıklık Matrisi ve Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# Sınıflandırma raporu\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"Combined Confusion Matrix and Classification Report saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "XmB3_WQ2M4Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#başarısız confusion matrix hesabı yaptı\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "# === 1. Paths ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "test_images_dir = \"/#your file path/train\"\n",
        "annotations_path = os.path.join(test_images_dir, \"_annotations_fixed.coco.json\")\n",
        "output_dir = \"/#your file path/test_outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Class names ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ===\n",
        "dataset_name = \"tekstil_val_v5\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Config setup ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3\n",
        "cfg.MODEL.DEVICE = \"cuda\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# === 5. Load annotations ===\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "\n",
        "gt_map = {}\n",
        "for ann in coco_ann[\"annotations\"]:\n",
        "    image_id = ann[\"image_id\"]\n",
        "    cat_id = ann[\"category_id\"]\n",
        "    gt_map.setdefault(image_id, []).append(cat_id)\n",
        "\n",
        "filename_to_id = {img[\"file_name\"]: img[\"id\"] for img in coco_ann[\"images\"]}\n",
        "\n",
        "# === 6. Prediction & saving ===\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for file in tqdm(os.listdir(test_images_dir)):\n",
        "    if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        continue\n",
        "\n",
        "    image_path = os.path.join(test_images_dir, file)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(file)\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for gt_class in gt_classes:\n",
        "        if pred_classes:\n",
        "            y_true.append(gt_class)\n",
        "            y_pred.append(pred_classes[0])\n",
        "        else:\n",
        "            y_true.append(gt_class)\n",
        "            y_pred.append(-1)  # No prediction\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, file)\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 7. Confusion Matrix ===\n",
        "labels = list(range(len(class_names)))\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels + [-1])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n"
      ],
      "metadata": {
        "id": "cQnYVxJuQsYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# İki ayrı test veri yolu\n",
        "test_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(test_dir_1, \"_annotations_fixed2.coco.json\")\n",
        "\n",
        "test_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(test_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "output_dir = \"/#your file path/combined_test_outputs33\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Sınıf İsimleri ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ve Config ===\n",
        "dataset_name = \"tekstil_combined_test\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"Model loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 4. Birleşik Ground Truth Verilerini Yükleme ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "def load_annotations_and_images(annotations_path, image_dir):\n",
        "    global gt_map, filename_to_id, all_image_paths\n",
        "\n",
        "    if not os.path.exists(annotations_path):\n",
        "        print(f\"Warning: Annotation file not found at {annotations_path}. Skipping this directory.\")\n",
        "        return\n",
        "\n",
        "    with open(annotations_path, \"r\") as f:\n",
        "        coco_ann = json.load(f)\n",
        "        for img in coco_ann[\"images\"]:\n",
        "            filename = img[\"file_name\"]\n",
        "            img_id = img[\"id\"]\n",
        "            full_path = os.path.join(image_dir, filename)\n",
        "\n",
        "            # Eğer aynı ID'ye sahip bir dosya zaten işlenmişse, atla\n",
        "            if img_id in gt_map:\n",
        "                continue\n",
        "\n",
        "            filename_to_id[filename] = img_id\n",
        "            all_image_paths.append(full_path)\n",
        "        for ann in coco_ann[\"annotations\"]:\n",
        "            gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "load_annotations_and_images(annotations_path_1, test_dir_1)\n",
        "load_annotations_and_images(annotations_path_2, test_dir_2)\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    # Tahminlerin ve ground truth'ların eşleşmesi için basitleştirilmiş döngü\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "\n",
        "        if i < len(pred_classes):\n",
        "            # Model 0'dan başlayan etiketler ürettiği için +1 ekliyoruz (COCO ID'si için)\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    # Fazla tahminleri (False Positives) de ekle\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    # Görseli kaydet\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Karmaşıklık Matrisi ve Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# Sınıflandırma raporu\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nCombined Confusion Matrix and Classification Report saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "9EYaHYjPRq03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# İki ayrı test veri yolu\n",
        "test_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(test_dir_1, \"_annotations_fixed2.coco.json\")\n",
        "\n",
        "test_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(test_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "output_dir = \"/#your file path/combined_test_outputs33\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Sınıf İsimleri ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ve Config ===\n",
        "dataset_name = \"tekstil_combined_test\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"Model loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 4. Birleşik Ground Truth Verilerini Yükleme ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "def load_annotations_and_images(annotations_path, image_dir):\n",
        "    global gt_map, filename_to_id, all_image_paths\n",
        "\n",
        "    if not os.path.exists(annotations_path):\n",
        "        print(f\"Warning: Annotation file not found at {annotations_path}. Skipping this directory.\")\n",
        "        return\n",
        "\n",
        "    with open(annotations_path, \"r\") as f:\n",
        "        coco_ann = json.load(f)\n",
        "        for img in coco_ann[\"images\"]:\n",
        "            filename = img[\"file_name\"]\n",
        "            img_id = img[\"id\"]\n",
        "            full_path = os.path.join(image_dir, filename)\n",
        "\n",
        "            # Eğer aynı ID'ye sahip bir dosya zaten işlenmişse, atla\n",
        "            if img_id in gt_map:\n",
        "                continue\n",
        "\n",
        "            filename_to_id[filename] = img_id\n",
        "            all_image_paths.append(full_path)\n",
        "        for ann in coco_ann[\"annotations\"]:\n",
        "            gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "load_annotations_and_images(annotations_path_1, test_dir_1)\n",
        "load_annotations_and_images(annotations_path_2, test_dir_2)\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    # Tahminlerin ve ground truth'ların eşleşmesi için basitleştirilmiş döngü\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    # Fazla tahminleri (False Positives) de ekle\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    # Görseli kaydet\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Karmaşıklık Matrisi ve Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# Sınıflandırma raporu\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nCombined Confusion Matrix and Classification Report saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "-bsv1m0SS0r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "# Birleştirilecek ilk veri seti klasörü ve annotation dosyası\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"_annotations_fixed.coco.json\")\n",
        "\n",
        "# Birleştirilecek ikinci veri seti klasörü ve annotation dosyası\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# Birleştirilmiş verilerin kaydedileceği hedef klasör\n",
        "combined_output_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_output_dir, \"combined_annotations.coco.json\")\n",
        "\n",
        "# Hedef klasörü oluştur (varsa silip yeniden oluşturur)\n",
        "if os.path.exists(combined_output_dir):\n",
        "    shutil.rmtree(combined_output_dir)\n",
        "os.makedirs(combined_output_dir, exist_ok=True)\n",
        "\n",
        "# --- 2. Birleştirme Fonksiyonu ---\n",
        "def combine_coco_datasets(paths):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleştiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"Uyarı: Annotation dosyası bulunamadı, atlanıyor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Kategorileri harita kullanarak ekle veya güncelle\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        # Resim ID'lerini ve annotation ID'lerini güncelle ve kopyala\n",
        "        image_id_mapping = {}\n",
        "        for image in tqdm(data['images'], desc=f\"Görseller kopyalanıyor: {img_dir}\"):\n",
        "            old_image_id = image['id']\n",
        "            new_image_id = old_image_id + image_id_offset\n",
        "            image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "            image_copy = image.copy()\n",
        "            image_copy['id'] = new_image_id\n",
        "\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "            dst_image_path = os.path.join(combined_output_dir, image['file_name'])\n",
        "            if os.path.exists(src_image_path):\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "            combined_images.append(image_copy)\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            ann_copy = annotation.copy()\n",
        "            ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "            ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "            # Kategori ID'lerini yeni haritaya göre güncelle\n",
        "            original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "            ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "            combined_annotations.append(ann_copy)\n",
        "\n",
        "        # Bir sonraki veri seti için offset'leri güncelle\n",
        "        image_id_offset += len(data['images'])\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": data.get(\"licenses\", []),\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "# --- 3. Birleştirme İşlemini Başlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "\n",
        "combined_dataset = combine_coco_datasets(paths_to_combine)\n",
        "\n",
        "with open(combined_annotations_path, \"w\") as f:\n",
        "    json.dump(combined_dataset, f, indent=4)\n",
        "\n",
        "print(f\"\\nBirleştirme tamamlandı. Görseller ve 'combined_annotations.coco.json' dosyası buraya kaydedildi: {combined_output_dir}\")"
      ],
      "metadata": {
        "id": "raWgK1a9Tg_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Birleştirilmiş veri setinin yolu\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "\n",
        "# Çıktıların kaydedileceği klasör\n",
        "output_dir = \"/#your file path/combined_analysis_report\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Sınıf İsimleri ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ve Config ===\n",
        "dataset_name = \"tekstil_combined_dataset\"\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    # Veri seti zaten kayıtlıysa hata vermesini engelle\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2  # Tahmin eşiği\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"Model loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 4. Ground Truth Verilerini Yükleme ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    # Tahminlerin ve ground truth'ların eşleşmesi için basitleştirilmiş döngü\n",
        "    # Not: Bu, her bir ground truth için bir tahmin eşleştirmeye çalışır.\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "\n",
        "        if i < len(pred_classes):\n",
        "            # Model 0'dan başlayan etiketler ürettiği için +1 ekliyoruz (COCO ID'si için)\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1) # Tahmin yoksa, \"None\" olarak işaretle\n",
        "\n",
        "    # Fazla tahminleri (False Positives) de ekle\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2) # Ekstra tahmin için özel bir etiket\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    # Görseli tahminlerle kaydet\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Karmaşıklık Matrisi ve Raporu ===\n",
        "# Etiketleri, 1'den başlayan sınıf ID'leri ve -1 (None) ile oluştur\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# Sınıflandırma raporu\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandı. Raporlar ve görseller buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "LyKiBQ1LTy8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yolları\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"_annotations_fixed2.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# Birleştirilmiş verilerin ve çıktıların kaydedileceği ana klasörler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report3\"\n",
        "\n",
        "# Klasörleri oluştur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri Kümelerini Birleştirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleştiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"Uyarı: Annotation dosyası bulunamadı, atlanıyor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        for image in tqdm(data['images'], desc=f\"Görseller kopyalanıyor: {img_dir}\"):\n",
        "            old_image_id = image['id']\n",
        "            new_image_id = old_image_id + image_id_offset\n",
        "            image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "            image_copy = image.copy()\n",
        "            image_copy['id'] = new_image_id\n",
        "\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "            dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "            if os.path.exists(src_image_path):\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "            combined_images.append(image_copy)\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            ann_copy = annotation.copy()\n",
        "            ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "            ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "            original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "            ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "            combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += len(data['images'])\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleştirme tamamlandı.\")\n",
        "\n",
        "# --- Birleştirme İşlemini Başlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz için Sınıf İsimleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "dataset_name = \"tekstil_combined_dataset\"\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model Yükleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Karmaşıklık Matrisi ve Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandı. Raporlar ve görseller buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "0TCsbpbjUdqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yolları\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# Birleştirilmiş verilerin ve çıktıların kaydedileceği ana klasörler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report22\"\n",
        "\n",
        "# Klasörleri oluştur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri Kümelerini Birleştirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleştiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"Uyarı: Annotation dosyası bulunamadı, atlanıyor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"Görseller kopyalanıyor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarı: {src_image_path} bulunamadı, bu görsel atlanıyor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasöründen {processed_count} görsel başarıyla kopyalandı, {skipped_count} görsel atlandı.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleştirme tamamlandı.\")\n",
        "\n",
        "# --- Birleştirme İşlemini Başlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz için Sınıf İsimleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model Yükleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Karmaşıklık Matrisi ve Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandı. Raporlar ve görseller buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "p3uRIh_4VGKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'ı bağlama (Colab için)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yolları\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# Birleştirilmiş verilerin ve çıktıların kaydedileceği ana klasörler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report33\"\n",
        "\n",
        "# Klasörleri oluştur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri Kümelerini Birleştirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleştiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"Uyarı: Annotation dosyası bulunamadı, atlanıyor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"Görseller kopyalanıyor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarı: {src_image_path} bulunamadı, bu görsel atlanıyor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasöründen {processed_count} görsel başarıyla kopyalandı, {skipped_count} görsel atlandı.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleştirme tamamlandı.\")\n",
        "\n",
        "# --- Birleştirme İşlemini Başlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "# === 3. Analiz için Sınıf İsimleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model Yükleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Karmaşıklık Matrisi ve Detaylı Metrik Raporu ===\n",
        "# Etiketleri, 1'den başlayan sınıf ID'leri ve -1 (None) ile oluştur\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "# Karmaşıklık Matrisi görselleştirmesini kaydet\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# Detaylı TP, TN, FP, FN değerlerini hesaplayıp raporla\n",
        "total_samples = len(y_true)\n",
        "report_content = f\"### Detaylı Metrik Raporu\\n\\nToplam Analiz Edilen Örnek Sayısı: {total_samples}\\n\\n\"\n",
        "\n",
        "for i, class_label in enumerate(disp_labels):\n",
        "    tp = cm[i, i]\n",
        "    fp = np.sum(cm[:, i]) - tp\n",
        "    fn = np.sum(cm[i, :]) - tp\n",
        "    tn = np.sum(cm) - (tp + fp + fn)\n",
        "\n",
        "    tp_percent = (tp / total_samples) * 100 if total_samples > 0 else 0\n",
        "    tn_percent = (tn / total_samples) * 100 if total_samples > 0 else 0\n",
        "    fp_percent = (fp / total_samples) * 100 if total_samples > 0 else 0\n",
        "    fn_percent = (fn / total_samples) * 100 if total_samples > 0 else 0\n",
        "\n",
        "    report_content += f\"--- {class_label} ---\\n\"\n",
        "    report_content += f\"TP (Doğru Pozitif): {tp} ({tp_percent:.2f}%)\\n\"\n",
        "    report_content += f\"TN (Doğru Negatif): {tn} ({tn_percent:.2f}%)\\n\"\n",
        "    report_content += f\"FP (Yanlış Pozitif): {fp} ({fp_percent:.2f}%)\\n\"\n",
        "    report_content += f\"FN (Yanlış Negatif): {fn} ({fn_percent:.2f}%)\\n\\n\"\n",
        "\n",
        "# Raporu dosyaya kaydet\n",
        "detailed_report_path = os.path.join(output_dir, \"detailed_metrics_report.txt\")\n",
        "with open(detailed_report_path, \"w\") as f:\n",
        "    f.write(report_content)\n",
        "\n",
        "# Sınıflandırma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandı. Raporlar ve görseller buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "hgvMEyZdYWa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'ı bağlama (Colab için)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yolları\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# Birleştirilmiş verilerin ve çıktıların kaydedileceği ana klasörler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report22\"\n",
        "\n",
        "# Klasörleri oluştur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri Kümelerini Birleştirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleştiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"Uyarı: Annotation dosyası bulunamadı, atlanıyor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"Görseller kopyalanıyor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarı: {src_image_path} bulunamadı, bu görsel atlanıyor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasöründen {processed_count} görsel başarıyla kopyalandı, {skipped_count} görsel atlandı.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleştirme tamamlandı.\")\n",
        "\n",
        "# --- Birleştirme İşlemini Başlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz için Sınıf İsimleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model Yükleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Karmaşıklık Matrisi ve Detaylı Metrik Raporu ===\n",
        "# Etiketleri, 1'den başlayan sınıf ID'leri ve -1 (None) ile oluştur\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "# Karmaşıklık Matrisi görselleştirmesini kaydet\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# Detaylı TP, TN, FP, FN değerlerini hesaplayıp görselleştirme\n",
        "total_samples = len(y_true)\n",
        "num_classes = len(disp_labels)\n",
        "metrics_data = np.zeros((num_classes, 4), dtype=int)\n",
        "metrics_percentage_data = np.zeros((num_classes, 4), dtype=float)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    tp = cm[i, i]\n",
        "    fp = np.sum(cm[:, i]) - tp\n",
        "    fn = np.sum(cm[i, :]) - tp\n",
        "    tn = np.sum(cm) - (tp + fp + fn)\n",
        "\n",
        "    metrics_data[i, 0] = tp\n",
        "    metrics_data[i, 1] = fp\n",
        "    metrics_data[i, 2] = fn\n",
        "    metrics_data[i, 3] = tn\n",
        "\n",
        "    tp_perc = (tp / total_samples) * 100 if total_samples > 0 else 0\n",
        "    fp_perc = (fp / total_samples) * 100 if total_samples > 0 else 0\n",
        "    fn_perc = (fn / total_samples) * 100 if total_samples > 0 else 0\n",
        "    tn_perc = (tn / total_samples) * 100 if total_samples > 0 else 0\n",
        "\n",
        "    metrics_percentage_data[i, 0] = tp_perc\n",
        "    metrics_percentage_data[i, 1] = fp_perc\n",
        "    metrics_percentage_data[i, 2] = fn_perc\n",
        "    metrics_percentage_data[i, 3] = tn_perc\n",
        "\n",
        "# Matrisi çizme\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "columns = ['TP (Doğru Pozitif)', 'FP (Yanlış Pozitif)', 'FN (Yanlış Negatif)', 'TN (Doğru Negatif)']\n",
        "ax.set_title(\"Detaylı Metrik Matrisi\")\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "cell_text = []\n",
        "for i in range(num_classes):\n",
        "    row_text = []\n",
        "    for j in range(4):\n",
        "        value = metrics_data[i, j]\n",
        "        percentage = metrics_percentage_data[i, j]\n",
        "        row_text.append(f\"{value}\\n({percentage:.2f}%)\")\n",
        "    cell_text.append(row_text)\n",
        "\n",
        "table = ax.table(cellText=cell_text, colLabels=columns, rowLabels=disp_labels, loc='center', cellLoc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1.2, 1.2)\n",
        "\n",
        "for (i, j), cell in table.get_celld().items():\n",
        "    if i == 0:  # Başlık satırı\n",
        "        cell.set_facecolor(\"#4CAF50\")\n",
        "    else:\n",
        "        # TP ve TN için yeşil, FP ve FN için kırmızı\n",
        "        if j == 0 or j == 3:\n",
        "            cell.set_facecolor(\"#E8F5E9\") # Açık yeşil\n",
        "        else:\n",
        "            cell.set_facecolor(\"#FFEBEE\") # Açık kırmızı\n",
        "\n",
        "plt.tight_layout()\n",
        "metrics_matrix_path = os.path.join(output_dir, \"detailed_metrics_matrix.png\")\n",
        "plt.savefig(metrics_matrix_path, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.close()\n",
        "\n",
        "# Sınıflandırma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandı. Raporlar ve görseller buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "ROJp2LXWb5pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "# Google Drive'ı bağlama (Colab için)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yolları\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# Birleştirilmiş verilerin ve çıktıların kaydedileceği ana klasörler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report55\"\n",
        "\n",
        "# Klasörleri oluştur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri Kümelerini Birleştirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleştiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"Uyarı: Annotation dosyası bulunamadı, atlanıyor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"Görseller kopyalanıyor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarı: {src_image_path} bulunamadı, bu görsel atlanıyor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasöründen {processed_count} görsel başarıyla kopyalandı, {skipped_count} görsel atlandı.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleştirme tamamlandı.\")\n",
        "\n",
        "# --- Birleştirme İşlemini Başlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz için Sınıf İsimleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model Yükleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Karmaşıklık Matrisi ve Detaylı Metrik Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "# Karmaşıklık Matrisi görselleştirmesini kaydet\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# Detaylı TP, TN, FP, FN değerlerini hesaplayıp görselleştirme\n",
        "total_samples = len(y_true)\n",
        "num_classes = len(disp_labels)\n",
        "\n",
        "metrics_data = np.zeros((num_classes, 4), dtype=int)\n",
        "metrics_percentage_data = np.zeros((num_classes, 4), dtype=float)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    tp = cm[i, i]\n",
        "    fp = np.sum(cm[:, i]) - tp\n",
        "    fn = np.sum(cm[i, :]) - tp\n",
        "    tn = np.sum(cm) - (tp + fp + fn)\n",
        "\n",
        "    metrics_data[i, 0] = tp\n",
        "    metrics_data[i, 1] = fp\n",
        "    metrics_data[i, 2] = fn\n",
        "    metrics_data"
      ],
      "metadata": {
        "id": "Ie40_BuMdHYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'ı bağlama (Colab için)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yolları\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# Birleştirilmiş verilerin ve çıktıların kaydedileceği ana klasörler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report66\"\n",
        "\n",
        "# Klasörleri oluştur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri Kümelerini Birleştirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleştiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"Uyarı: Annotation dosyası bulunamadı, atlanıyor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"Görseller kopyalanıyor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarı: {src_image_path} bulunamadı, bu görsel atlanıyor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasöründen {processed_count} görsel başarıyla kopyalandı, {skipped_count} görsel atlandı.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleştirme tamamlandı.\")\n",
        "\n",
        "# --- Birleştirme İşlemini Başlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz için Sınıf İsimleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model Yükleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Genel Başarı Matrisi (TP/FP/FN/TN) Oluşturma ===\n",
        "# Tüm defect sınıflarını \"Hatalı\" olarak, None ve Diğer sınıfları \"Hatasız\" olarak kabul etme\n",
        "defect_class_ids = set(range(1, len(class_names) + 1))\n",
        "y_true_binary = [1 if val in defect_class_ids else 0 for val in y_true]\n",
        "y_pred_binary = [1 if val in defect_class_ids else 0 for val in y_pred]\n",
        "\n",
        "# 2x2 karışıklık matrisini hesapla\n",
        "cm = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "total_samples = tn + fp + fn + tp\n",
        "\n",
        "# Matrisi istenen düzende görselleştirme\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.axis('off')\n",
        "ax.set_title('Genel Başarı Matrisi', fontsize=18, pad=20)\n",
        "\n",
        "# Verileri ve etiketleri hazırlama\n",
        "cell_texts = np.array([\n",
        "    [f\"TP: {tp}\\n({tp/total_samples*100:.2f}%)\", f\"FP: {fp}\\n({fp/total_samples*100:.2f}%)\"],\n",
        "    [f\"FN: {fn}\\n({fn/total_samples*100:.2f}%)\", f\"TN: {tn}\\n({tn/total_samples*100:.2f}%)\"]\n",
        "])\n",
        "row_labels = [\"Gerçek Hatalı (Expected)\", \"Gerçek Hatasız\"]\n",
        "col_labels = [\"Tahmin Edilen Hatalı (Predicted)\", \"Tahmin Edilen Hatasız\"]\n",
        "\n",
        "# Matrisi çizme\n",
        "table = ax.table(cellText=cell_texts,\n",
        "                 rowLabels=row_labels,\n",
        "                 colLabels=col_labels,\n",
        "                 loc='center',\n",
        "                 cellLoc='center')\n",
        "\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "table.scale(1.2, 1.2)\n",
        "\n",
        "# Renklendirme\n",
        "table.get_celld()[1, 0].set_facecolor('#d9f1e1')  # TP\n",
        "table.get_celld()[1, 1].set_facecolor('#f8e2e2')  # FP\n",
        "table.get_celld()[0, 0].set_facecolor('#f8e2e2')  # FN\n",
        "table.get_celld()[0, 1].set_facecolor('#d9f1e1')  # TN\n",
        "\n",
        "plt.tight_layout()\n",
        "matrix_path = os.path.join(output_dir, \"genel_basari_matrisi.png\")\n",
        "plt.savefig(matrix_path, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.close()\n",
        "\n",
        "# Sınıflandırma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandı. Genel Başarı Matrisi buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "tkMrS51GfLLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'ı bağlama (Colab için)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yolları\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# Birleştirilmiş verilerin ve çıktıların kaydedileceği ana klasörler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report666\"\n",
        "\n",
        "# Klasörleri oluştur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri Kümelerini Birleştirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleştiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"Uyarı: Annotation dosyası bulunamadı, atlanıyor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"Görseller kopyalanıyor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarı: {src_image_path} bulunamadı, bu görsel atlanıyor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasöründen {processed_count} görsel başarıyla kopyalandı, {skipped_count} görsel atlandı.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleştirme tamamlandı.\")\n",
        "\n",
        "# --- Birleştirme İşlemini Başlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz için Sınıf İsimleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model Yükleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Genel Başarı Matrisi (TP/FP/FN/TN) Oluşturma ===\n",
        "# Tüm defect sınıflarını \"Hatalı\" olarak, None ve Diğer sınıfları \"Hatasız\" olarak kabul etme\n",
        "defect_class_ids = set(range(1, len(class_names) + 1))\n",
        "y_true_binary = [1 if val in defect_class_ids else 0 for val in y_true]\n",
        "y_pred_binary = [1 if val in defect_class_ids else 0 for val in y_pred]\n",
        "\n",
        "# 2x2 karışıklık matrisini hesapla\n",
        "cm = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "total_samples = tn + fp + fn + tp\n",
        "\n",
        "# Matrisi istenen düzende görselleştirme\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.set_title('Genel Başarı Matrisi', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Tahmin Edilen', fontsize=14)\n",
        "ax.set_ylabel('Gerçek', fontsize=14)\n",
        "\n",
        "# Renk haritası için veri\n",
        "heatmap_data = np.array([[tp, fp], [fn, tn]])\n",
        "normalized_data = heatmap_data / np.max(heatmap_data) if np.max(heatmap_data) > 0 else np.zeros((2,2))\n",
        "\n",
        "# Renklendirme\n",
        "cmap = plt.cm.get_cmap('Greens')\n",
        "cmap_r = plt.cm.get_cmap('Reds')\n",
        "\n",
        "# TP ve TN için yeşil tonları, FP ve FN için kırmızı tonları\n",
        "colors = np.zeros((2, 2, 4))\n",
        "colors[0, 0] = cmap(normalized_data[0, 0]) # TP\n",
        "colors[0, 1] = cmap_r(normalized_data[0, 1]) # FP\n",
        "colors[1, 0] = cmap_r(normalized_data[1, 0]) # FN\n",
        "colors[1, 1] = cmap(normalized_data[1, 1]) # TN\n",
        "\n",
        "# Matrisin kendisi\n",
        "ax.imshow(colors, interpolation='nearest', origin='upper')\n",
        "\n",
        "# Metinleri hücrelere yazma\n",
        "labels_text = [\n",
        "    [f\"TP\\n{tp}\\n({tp/total_samples*100:.2f}%)\", f\"FP\\n{fp}\\n({fp/total_samples*100:.2f}%)\"],\n",
        "    [f\"FN\\n{fn}\\n({fn/total_samples*100:.2f}%)\", f\"TN\\n{tn}\\n({tn/total_samples*100:.2f}%)\"]\n",
        "]\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, labels_text[i][j], ha='center', va='center', color='black', fontsize=16, weight='bold')\n",
        "\n",
        "# Eksen etiketlerini ve işaretlerini ayarla\n",
        "ax.set_xticks(np.arange(2))\n",
        "ax.set_yticks(np.arange(2))\n",
        "ax.set_xticklabels([\"Hatalı\", \"Hatasız\"])\n",
        "ax.set_yticklabels([\"Hatalı\", \"Hatasız\"])\n",
        "\n",
        "plt.tight_layout()\n",
        "matrix_path = os.path.join(output_dir, \"genel_basari_matrisi.png\")\n",
        "plt.savefig(matrix_path, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.close()\n",
        "\n",
        "# Sınıflandırma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandı. Genel Başarı Matrisi buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "tRjqs5-bgK8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'ı bağlama (Colab için)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yolları\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# Birleştirilmiş verilerin ve çıktıların kaydedileceği ana klasörler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report6666\"\n",
        "\n",
        "# Klasörleri oluştur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri Kümelerini Birleştirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleştiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"Uyarı: Annotation dosyası bulunamadı, atlanıyor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"Görseller kopyalanıyor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarı: {src_image_path} bulunamadı, bu görsel atlanıyor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasöründen {processed_count} görsel başarıyla kopyalandı, {skipped_count} görsel atlandı.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleştirme tamamlandı.\")\n",
        "\n",
        "# --- Birleştirme İşlemini Başlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz için Sınıf İsimleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model Yükleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Genel Başarı Matrisi (TP/FP/FN/TN) Oluşturma ===\n",
        "\n",
        "# Tüm defect sınıflarını \"Hatalı\" olarak, None ve Diğer sınıfları \"Hatasız\" olarak kabul etme\n",
        "defect_class_ids = set(range(1, len(class_names) + 1))\n",
        "y_true_binary = [1 if val in defect_class_ids else 0 for val in y_true]\n",
        "y_pred_binary = [1 if val in defect_class_ids else 0 for val in y_pred]\n",
        "\n",
        "# 2x2 karışıklık matrisini hesapla\n",
        "cm = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "total_samples = tn + fp + fn + tp\n",
        "\n",
        "# Matrisi istenen düzende görselleştirme\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.set_title('Genel Başarı Matrisi', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Tahmin Edilen', fontsize=14)\n",
        "ax.set_ylabel('Gerçek', fontsize=14)\n",
        "\n",
        "# Renk haritası için veri\n",
        "heatmap_data = np.array([[tp, fp], [fn, tn]])\n",
        "normalized_data = heatmap_data / np.max(heatmap_data) if np.max(heatmap_data) > 0 else np.zeros((2,2))\n",
        "\n",
        "# Renklendirme\n",
        "cmap_green = plt.cm.get_cmap('Greens')\n",
        "cmap_red = plt.cm.get_cmap('Reds')\n",
        "\n",
        "# TP ve TN için yeşil tonları, FP ve FN için kırmızı tonları\n",
        "colors = np.zeros((2, 2, 4))\n",
        "colors[0, 0] = cmap_green(normalized_data[0, 0]) # TP\n",
        "colors[0, 1] = cmap_red(normalized_data[0, 1])   # FP\n",
        "colors[1, 0] = cmap_red(normalized_data[1, 0])   # FN\n",
        "colors[1, 1] = cmap_green(normalized_data[1, 1]) # TN\n",
        "\n",
        "# Matrisin kendisi\n",
        "ax.imshow(colors, interpolation='nearest', origin='upper')\n",
        "\n",
        "# Metinleri hücrelere yazma\n",
        "labels_text = [\n",
        "    [f\"TP\\n{tp}\\n({tp/total_samples*100:.2f}%)\", f\"FP\\n{fp}\\n({fp/total_samples*100:.2f}%)\"],\n",
        "    [f\"FN\\n{fn}\\n({fn/total_samples*100:.2f}%)\", f\"TN\\n{tn}\\n({tn/total_samples*100:.2f}%)\"]\n",
        "]\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, labels_text[i][j], ha='center', va='center', color='black', fontsize=16, weight='bold')\n",
        "\n",
        "# Eksen etiketlerini ve işaretlerini ayarla\n",
        "ax.set_xticks(np.arange(2))\n",
        "ax.set_yticks(np.arange(2))\n",
        "ax.set_xticklabels([\"Hatalı\", \"Hatasız\"])\n",
        "ax.set_yticklabels([\"Hatalı\", \"Hatasız\"])\n",
        "\n",
        "plt.tight_layout()\n",
        "matrix_path = os.path.join(output_dir, \"genel_basari_matrisi.png\")\n",
        "plt.savefig(matrix_path, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.close()\n",
        "\n",
        "# Sınıflandırma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true_binary, y_pred_binary, labels=[0, 1],\n",
        "    target_names=[\"Hatasız\", \"Hatalı\"],\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"binary_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandı. Genel Başarı Matrisi buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "5jZnta0ChocP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'ı bağlama (Colab için)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya Yolları ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yolları\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# Birleştirilmiş verilerin ve çıktıların kaydedileceği ana klasörler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report66666\"\n",
        "\n",
        "# Klasörleri oluştur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri Kümelerini Birleştirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleştiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"Uyarı: Annotation dosyası bulunamadı, atlanıyor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"Görseller kopyalanıyor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarı: {src_image_path} bulunamadı, bu görsel atlanıyor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasöründen {processed_count} görsel başarıyla kopyalandı, {skipped_count} görsel atlandı.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleştirme tamamlandı.\")\n",
        "\n",
        "# --- Birleştirme İşlemini Başlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz için Sınıf İsimleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model Yükleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve Karşılaştırma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# --- 6. Genel Başarı Matrisi (TP/FP/FN/TN) Oluşturma ---\n",
        "\n",
        "# Tüm defect sınıflarını \"Hatalı\" olarak, None ve Diğer sınıfları \"Hatasız\" olarak kabul etme\n",
        "defect_class_ids = set(range(1, len(class_names) + 1))\n",
        "y_true_binary = [1 if val in defect_class_ids else 0 for val in y_true]\n",
        "y_pred_binary = [1 if val in defect_class_ids else 0 for val in y_pred]\n",
        "\n",
        "# 2x2 karışıklık matrisini hesapla\n",
        "cm = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "total_samples = tn + fp + fn + tp\n",
        "\n",
        "# Matrisi istenen düzende görselleştirme\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.set_title('Genel Başarı Matrisi', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Tahmin Edilen', fontsize=14)\n",
        "ax.set_ylabel('Gerçek', fontsize=14)\n",
        "\n",
        "# Renk haritası için veri\n",
        "heatmap_data = np.array([[tp, fp], [fn, tn]])\n",
        "normalized_data = heatmap_data / np.max(heatmap_data) if np.max(heatmap_data) > 0 else np.zeros((2,2))\n",
        "\n",
        "# Tek bir mavi renk tonu kullanma\n",
        "cmap = plt.cm.get_cmap('Blues')\n",
        "ax.imshow(normalized_data, cmap=cmap, interpolation='nearest', origin='upper')\n",
        "\n",
        "# Metinleri hücrelere yazma\n",
        "labels_text = [\n",
        "    [f\"TP\\n{tp}\\n({tp/total_samples*100:.2f}%)\", f\"FP\\n{fp}\\n({fp/total_samples*100:.2f}%)\"],\n",
        "    [f\"FN\\n{fn}\\n({fn/total_samples*100:.2f}%)\", f\"TN\\n{tn}\\n({tn/total_samples*100:.2f}%)\"]\n",
        "]\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        color = 'white' if normalized_data[i, j] > 0.5 else 'black'\n",
        "        ax.text(j, i, labels_text[i][j], ha='center', va='center', color=color, fontsize=16, weight='bold')\n",
        "\n",
        "# Eksen etiketlerini ve işaretlerini ayarla\n",
        "ax.set_xticks(np.arange(2))\n",
        "ax.set_yticks(np.arange(2))\n",
        "ax.set_xticklabels([\"Hatalı\", \"Hatasız\"])\n",
        "ax.set_yticklabels([\"Hatalı\", \"Hatasız\"])\n",
        "\n",
        "plt.tight_layout()\n",
        "matrix_path = os.path.join(output_dir, \"genel_basari_matrisi.png\")\n",
        "plt.savefig(matrix_path, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.close()\n",
        "\n",
        "# Sınıflandırma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true_binary, y_pred_binary, labels=[0, 1],\n",
        "    target_names=[\"Hatasız\", \"Hatalı\"],\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"binary_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandı. Genel Başarı Matrisi buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "lvZedlnwjTcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive'ı bağlama (Colab için)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === Dosya Yolları ===\n",
        "# Taşınacak kaynak dosya (yeni konumu)\n",
        "source_file = \"/#your file path/ikinci.coco.json\"\n",
        "\n",
        "# Dosyanın eski konumu (hedef klasör)\n",
        "destination_dir = \"/#your file path/train\"\n",
        "destination_path = os.path.join(destination_dir, \"ikinci.coco.json\")\n",
        "\n",
        "# === Dosyayı Taşıma ===\n",
        "if os.path.exists(source_file):\n",
        "    try:\n",
        "        shutil.move(source_file, destination_path)\n",
        "        print(f\"'{source_file}' dosyası başarıyla '{destination_dir}' klasörüne geri taşındı.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Dosyayı taşırken bir hata oluştu: {e}\")\n",
        "else:\n",
        "    print(f\"Hata: '{source_file}' dosyası belirtilen yolda bulunamadı.\")"
      ],
      "metadata": {
        "id": "g9kt54qUW3pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Google Drive'ı bağlama (Colab için)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === Dosya Yolları ===\n",
        "# Taşınacak kaynak dosyalar\n",
        "source_file_1 = \"/#your file path/ikinci.coco.json\"\n",
        "source_file_2 = \"/#your file path/train/ilk.coco.json\"\n",
        "\n",
        "# Dosyaların taşınacağı hedef klasör.\n",
        "destination_dir = \"/content/drive/MyDrive/jsonss\"\n",
        "\n",
        "# === 1. Hedef Klasörü Oluşturma ===\n",
        "# Eğer 'jsonss' klasörü yoksa, oluştur.\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "print(f\"Hedef klasör hazır: {destination_dir}\\n\")\n",
        "\n",
        "# === 2. Dosyaları Taşıma ===\n",
        "files_to_move = [source_file_1, source_file_2]\n",
        "\n",
        "for file_path in files_to_move:\n",
        "    # Dosya adını al\n",
        "    file_name = os.path.basename(file_path)\n",
        "\n",
        "    # Yeni dosya yolunu oluştur\n",
        "    destination_path = os.path.join(destination_dir, file_name)\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            # Dosyayı taşı\n",
        "            shutil.move(file_path, destination_path)\n",
        "            print(f\"'{file_name}' dosyası başarıyla taşındı.\")\n",
        "        except Exception as e:\n",
        "            print(f\"'{file_name}' dosyasını taşırken bir hata oluştu: {e}\")\n",
        "    else:\n",
        "        print(f\"Hata: '{file_name}' dosyası kaynak yolda bulunamadı.\")\n",
        "\n",
        "print(\"\\nTaşıma işlemi tamamlandı.\")"
      ],
      "metadata": {
        "id": "SUbw68_sWIg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "ann_path = \"/#your file path3/train/_annotations.coco.json\"\n",
        "print(\"Dosya var mı?\", os.path.exists(ann_path))\n"
      ],
      "metadata": {
        "id": "tKxbHZ31By72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2 import _C   # detectron2 nin yol uyumsuzlugu çıkınca kullandığım kod\n",
        "\n",
        "print(_C.__file__)\n"
      ],
      "metadata": {
        "id": "4-ajPzHj-rAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerekli kütüphaneleri içe aktarma\n",
        "import torch\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "import os\n",
        "import json\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "# Google Drive'ı bağlama\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Logger'ı ayarlama\n",
        "setup_logger()\n",
        "\n",
        "# 1. Eğitilmiş Modeli ve Konfigürasyon Dosyalarını Yükleme\n",
        "print(\"Eğitilmiş model dosyaları yükleniyor...\")\n",
        "\n",
        "# Model ve konfigürasyon dosyalarının yolları\n",
        "model_dir = \"/#your file path/model1\"\n",
        "model_path = os.path.join(model_dir, \"model_final.pth\")\n",
        "config_path = os.path.join(model_dir, \"config.yml\")\n",
        "\n",
        "# Konfigürasyon dosyasını yükle\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "\n",
        "# Eğitilmiş modelin ağırlıklarını yükle\n",
        "cfg.MODEL.WEIGHTS = model_path\n",
        "\n",
        "# Düşük güvenilirlikteki tahminleri filtrelemek için eşik değerini düşürdük.\n",
        "# Bu, modelin daha fazla nesne bulmasını sağlayacaktır.\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3  # Hassasiyet düşürüldü.\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Eğitilmiş model ile tahmin yapmak için bir öngörücü (predictor) oluştur\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# 2. Resimleri Analiz Etme ve Görselleştirme\n",
        "print(\"Foto-Arsivi/lab_data2_gray klasöründeki fotoğraflar analiz ediliyor...\")\n",
        "\n",
        "# Analiz edilecek resimlerin bulunduğu klasörün yeni yolu\n",
        "test_image_dir = \"/#your file path/lab_data2_gray\"\n",
        "\n",
        "# JPG ve PNG uzantılı tüm dosyaları listele\n",
        "image_files = [f for f in os.listdir(test_image_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
        "\n",
        "if not image_files:\n",
        "    print(\"Hata: Belirtilen klasörde hiç JPG veya PNG dosyası bulunamadı. Lütfen yolu ve dosya uzantısını kontrol edin.\")\n",
        "else:\n",
        "    # Her bir resim dosyası için tahmin yap\n",
        "    for filename in image_files:\n",
        "        image_path = os.path.join(test_image_dir, filename)\n",
        "\n",
        "        img = cv2.imread(image_path)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Hata: {filename} dosyası okunamadı. Atlama yapılıyor.\")\n",
        "            continue\n",
        "\n",
        "        outputs = predictor(img)\n",
        "\n",
        "        # Görselleştirme için Visualizer oluştur\n",
        "        v = Visualizer(img[:, :, ::-1], scale=1.0)\n",
        "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "        print(f\"Analiz edilen dosya: {filename}\")\n",
        "        cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "\n",
        "print(\"\\nNesne tespiti ve görselleştirme tamamlandı.\")\n"
      ],
      "metadata": {
        "id": "fn1PTMuT05o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Google Drive'ı bağla\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Yolları ayarla\n",
        "model_dir = \"/#your file path/model3\"\n",
        "test_dir = \"/#your file path/test\"\n",
        "test_json = os.path.join(test_dir, \"_annotations.coco.json\")\n",
        "gray_dir = \"/#your file path/lab_data2_gray\"  # Örnek etiket yok klasör\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/Detectron2_Results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "test_vis_dir = os.path.join(output_dir, \"test_visualizations\")\n",
        "gray_vis_dir = os.path.join(output_dir, \"gray_visualizations\")\n",
        "os.makedirs(test_vis_dir, exist_ok=True)\n",
        "os.makedirs(gray_vis_dir, exist_ok=True)\n",
        "\n",
        "# 3. Test datasını COCO olarak kayıt et\n",
        "dataset_name = \"tekstil_test_v2\"\n",
        "register_coco_instances(dataset_name, {}, test_json, test_dir)\n",
        "\n",
        "# 4. COCO JSON'dan sınıf isimlerini çek ve metadata'ya ver\n",
        "with open(test_json, \"r\") as f:\n",
        "    coco_data = json.load(f)\n",
        "thing_classes = [cat[\"name\"] for cat in coco_data[\"categories\"]]\n",
        "MetadataCatalog.get(dataset_name).thing_classes = thing_classes\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# 5. Detectron2 modeli yükle ve ayarla\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(os.path.join(model_dir, \"config.yml\"))\n",
        "cfg.MODEL.WEIGHTS = os.path.join(model_dir, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2  # Hassasiyet artırıldı (eşik düşürüldü)\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# 6. Test görüntülerinde tahmin yap, görselleştir ve confusion matrix için gerçek ve tahminleri topla\n",
        "dataset_dicts = DatasetCatalog.get(dataset_name)\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "print(\"Test görüntüleri üzerinde tahmin ve confusion matrix hesaplanıyor...\")\n",
        "for data in dataset_dicts:\n",
        "    img_path = data[\"file_name\"]\n",
        "    img = cv2.imread(img_path)\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "    # Gerçek sınıflar (etiketler)\n",
        "    gt_classes = [ann[\"category_id\"] for ann in data[\"annotations\"]]\n",
        "    # Tahmin edilen sınıflar\n",
        "    pred_classes = instances.pred_classes.tolist()\n",
        "\n",
        "    y_true.extend(gt_classes)\n",
        "    y_pred.extend(pred_classes)\n",
        "\n",
        "    # Görselleştir ve kaydet\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    save_path = os.path.join(test_vis_dir, os.path.basename(img_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "print(\"Test görselleri tahminleri ve görselleri kaydedildi.\")\n",
        "\n",
        "# Confusion matrix çiz\n",
        "labels = list(range(len(thing_classes)))\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=thing_classes)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "disp.plot(xticks_rotation=45, cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix - Test Set\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.show()\n",
        "print(f\"Confusion matrix görseli kaydedildi: {cm_path}\")\n",
        "\n",
        "# 7. Etiketsiz gray klasöründeki görüntüler için sadece tahmin yap ve görsel kaydet\n",
        "print(\"\\nEtiketsiz (gray) klasöründeki görüntüler tahmin ediliyor ve kaydediliyor...\")\n",
        "\n",
        "gray_images = [f for f in os.listdir(gray_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
        "\n",
        "for filename in gray_images:\n",
        "    img_path = os.path.join(gray_dir, filename)\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"Uyarı: {filename} okunamadı, atlanıyor.\")\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    save_path = os.path.join(gray_vis_dir, filename)\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "print(\"Gray klasöründeki görseller işlenip kaydedildi.\")\n",
        "print(\"\\nTüm işlem tamamlandı.\")\n"
      ],
      "metadata": {
        "id": "5Z-GdRRH7_4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# JSON dosya yolu\n",
        "json_path = \"/#your file path/test/_annotations.coco.json\"\n",
        "\n",
        "# JSON dosyasını aç ve içeriğini yükle\n",
        "with open(json_path, \"r\") as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "# \"categories\" listesinden sınıf isimlerini çıkar\n",
        "thing_classes = [cat[\"name\"] for cat in coco_data[\"categories\"]]\n",
        "\n",
        "# Sonuçları yazdır\n",
        "print(\"Sınıf isimleri (thing_classes):\", thing_classes)\n"
      ],
      "metadata": {
        "id": "Hwmx97A18WtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJ4NPFHwYv7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerekli kütüphaneleri içe aktarma\n",
        "import cv2\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive'ı bağlama\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. Klasör yollarını ve çıktı ayarlarını belirleme\n",
        "# LÜTFEN BU YOLLARIN VE AYARLARIN DOĞRU OLDUĞUNDAN EMİN OLUN!\n",
        "\n",
        "# Analiz edilecek resimlerin bulunduğu kaynak klasör\n",
        "source_dir = \"/#your file path/lab_data2_HD\"\n",
        "\n",
        "# İşlenmiş resimlerin kaydedileceği hedef klasör\n",
        "# Bu klasör otomatik olarak oluşturulacaktır.\n",
        "destination_dir = \"/#your file path/lab_data2_gray\"\n",
        "\n",
        "# Resimlerin küçültüleceği hedef boyut (genişlik, yükseklik)\n",
        "# Örnek: (640, 480) veya (1280, 720) gibi bir değer girebilirsiniz.\n",
        "target_size = (640, 480)\n",
        "\n",
        "# 2. Klasörlerin varlığını kontrol etme ve oluşturma\n",
        "\n",
        "# Kaynak klasörün varlığını kontrol et\n",
        "if not os.path.exists(source_dir):\n",
        "    print(f\"Hata: Kaynak klasör bulunamadı: {source_dir}\")\n",
        "    exit()\n",
        "\n",
        "# Hedef klasörü oluştur\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "print(f\"Hedef klasör oluşturuldu veya zaten mevcut: {destination_dir}\\n\")\n",
        "\n",
        "# 3. Resimleri işleme ve kaydetme\n",
        "\n",
        "print(\"Resimler işlenmeye başlıyor...\")\n",
        "\n",
        "# Kaynak klasördeki tüm dosyaları döngüye al\n",
        "for filename in os.listdir(source_dir):\n",
        "    # Dosyanın PNG formatında olup olmadığını kontrol et\n",
        "    if filename.lower().endswith('.png'):\n",
        "        source_path = os.path.join(source_dir, filename)\n",
        "\n",
        "        # Resmi oku (OpenCV, resimleri varsayılan olarak BGR formatında okur)\n",
        "        img = cv2.imread(source_path)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Hata: {filename} dosyası okunamadı. Atlama yapılıyor.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Resim boyutunu küçültme\n",
        "            resized_img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # Yeni dosya adı ve yolu oluşturma\n",
        "            # .png uzantısını .jpg ile değiştir\n",
        "            new_filename = os.path.splitext(filename)[0] + '.jpg'\n",
        "            destination_path = os.path.join(destination_dir, new_filename)\n",
        "\n",
        "            # Boyutu küçültülmüş renkli resmi JPG olarak kaydetme\n",
        "            # Artık gri tonlamaya dönüştürme işlemi yapılmıyor.\n",
        "            cv2.imwrite(destination_path, resized_img)\n",
        "\n",
        "            print(f\"'{filename}' başarıyla işlendi ve '{new_filename}' olarak kaydedildi.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Hata: '{filename}' dosyası işlenirken bir sorun oluştu. Hata: {e}\")\n",
        "\n",
        "print(\"\\nTüm resimler başarıyla işlendi ve kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "rWHPdjfh0Sx5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}