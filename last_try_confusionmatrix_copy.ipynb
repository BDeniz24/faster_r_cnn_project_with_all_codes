{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP85lWrSwksaQTX/jjXKFaa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWUeLDkm5nok",
        "outputId": "98e3255c-88b2-4e2e-9aeb-164ee6b2a153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "# 2. DosyayÄ± yÃ¼kle (seÃ§meli)\n",
        "uploaded = files.upload()   # colabe dosyaa yÃ¼kleme\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "U5X9LY_K8W6v",
        "outputId": "7c955ee3-3751-4cc3-9e53-25e8bbee88e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e4a3293d-c0ce-47e8-b7d3-0ea74eba69e7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e4a3293d-c0ce-47e8-b7d3-0ea74eba69e7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving tekstil3test.zip to tekstil3test.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#colabde yÃ¼klÃ¼ dosyayÄ± baÅŸka yere taÅŸÄ±ma\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "# 3. Her yÃ¼klenen dosyayÄ± Drive'daki aynÄ± adlÄ± klasÃ¶re taÅŸÄ±\n",
        "for filename in uploaded.keys():\n",
        "    # Drive'daki hedef klasÃ¶r yolu (Ã¶rnek: MyDrive/tekstil3test)\n",
        "    drive_folder = f\"#your file path{filename.split('.')[0]}\"  # klasÃ¶r adÄ±: dosya adÄ± (uzantÄ±sÄ±z)\n",
        "\n",
        "    # EÄŸer klasÃ¶r yoksa oluÅŸtur\n",
        "    if not os.path.exists(drive_folder):\n",
        "        os.makedirs(drive_folder)\n",
        "\n",
        "    # YÃ¼klenen dosyanÄ±n geÃ§ici yolu\n",
        "    src_path = f\"/content/{filename}\"\n",
        "\n",
        "    # Drive'daki hedef dosya yolu\n",
        "    dest_path = os.path.join(drive_folder, filename)\n",
        "\n",
        "    # DosyayÄ± taÅŸÄ±\n",
        "    shutil.move(src_path, dest_path)\n",
        "\n",
        "    print(f\"{filename} dosyasÄ± Drive'daki {drive_folder} klasÃ¶rÃ¼ne taÅŸÄ±ndÄ±.\")"
      ],
      "metadata": {
        "id": "KrBAok1h8l5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# colabde yÃ¼klÃ¼ dosyayÄ± bir klsÃ¶rde aÃ§ma zipli olan dosyayÄ±\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "drive_folder = \"#your file path\"  # aÃ§mak istediÄŸin klasÃ¶r yolu\n",
        "zip_path = os.path.join(drive_folder, \"tekstil3test.zip\")  # zip dosyanÄ±n tam adÄ±\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(drive_folder)\n",
        "\n",
        "print(\"Zip dosyasÄ± baÅŸarÄ±yla aÃ§Ä±ldÄ±.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKcJ1frs83yU",
        "outputId": "7b5f9853-b05a-44ae-f195-de7070dece88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip dosyasÄ± baÅŸarÄ±yla aÃ§Ä±ldÄ±.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# faster r cnn ile eÄŸtitirken oluÅŸan bir json annotations dosyasÄ±nÄ±nÄ±n json da categories ksÄ±mÄ±nÄ± deÄŸiÅŸtirme\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Dosya yolu (train annotations dosyasÄ±)\n",
        "train_ann_path = \"#your file path/train/_annotations.coco.json\"\n",
        "\n",
        "# EÄŸitimdeki sÄ±nÄ±flar (supercategory olmadan)\n",
        "categories_new = [\n",
        "    {\"id\": 0, \"name\": \"defect\"},\n",
        "    {\"id\": 1, \"name\": \"1- patlak\"},\n",
        "    {\"id\": 2, \"name\": \"2- igne_kirigi\"},\n",
        "    {\"id\": 3, \"name\": \"3- jut\"},\n",
        "    {\"id\": 4, \"name\": \"5- likra_kacigi\"},\n",
        "    {\"id\": 5, \"name\": \"6- yag_lekesi\"},\n",
        "    {\"id\": 6, \"name\": \"8- May cizgisi\"},\n",
        "    {\"id\": 7, \"name\": \"fsa\"}   # fsa default olarak eklenen class adÄ±\n",
        "]\n",
        "\n",
        "# JSON dosyasÄ±nÄ± aÃ§\n",
        "with open(train_ann_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Mevcut categories'den id:name map\n",
        "old_cat_id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in data[\"categories\"]}\n",
        "\n",
        "# Yeni categories name:id map (kolay eÅŸleme iÃ§in)\n",
        "new_cat_name_to_id = {cat[\"name\"]: cat[\"id\"] for cat in categories_new}\n",
        "\n",
        "# Annotationlardaki category_id'leri yeni id'lere eÅŸle\n",
        "for ann in data[\"annotations\"]:\n",
        "    old_id = ann[\"category_id\"]\n",
        "    old_name = old_cat_id_to_name.get(old_id)\n",
        "    if old_name is None:\n",
        "        print(f\"UyarÄ±: Eski ID {old_id} iÃ§in isim bulunamadÄ±!\")\n",
        "        continue\n",
        "    new_id = new_cat_name_to_id.get(old_name)\n",
        "    if new_id is None:\n",
        "        print(f\"UyarÄ±: Yeni kategori listesinde '{old_name}' yok!\")\n",
        "        continue\n",
        "    ann[\"category_id\"] = new_id\n",
        "\n",
        "# categories kÄ±smÄ±nÄ± tamamen gÃ¼ncelle, supercategory olmadan\n",
        "data[\"categories\"] = categories_new\n",
        "\n",
        "# DosyayÄ± kaydet (istersen farklÄ± isimde)\n",
        "fixed_train_ann_path = \"#your file path/train/_annotations_fixed.coco.json\"\n",
        "with open(fixed_train_ann_path, \"w\") as f:\n",
        "    json.dump(data, f, indent=4)\n",
        "\n",
        "print(f\"Train annotation dosyasÄ± gÃ¼ncellendi ve {fixed_train_ann_path} olarak kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "d3X449IZ9vBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Dosya yolu (Ã¶rnek: train klasÃ¶rÃ¼ndeki dosya)\n",
        "json_path = \"#your file path/train/ikinci.coco.json\"\n",
        "\n",
        "# DosyayÄ± oku\n",
        "with open(json_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Kategorileri yazdÄ±r\n",
        "print(\"ðŸ“¦ Categories:\")\n",
        "for cat in data.get(\"categories\", []):\n",
        "    print(f\"ðŸ†” {cat['id']} - {cat['name']}\")\n"
      ],
      "metadata": {
        "id": "A0NDOJVf98PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugf7RzVPxx_x"
      },
      "outputs": [],
      "source": [
        "#FASTER R CNN Ä°Ã‡Ä°N PAKET VE DOSYA KURULUMU\n",
        "\n",
        "\n",
        "# Mevcut Colab ortamÄ±ndaki PyTorch ve diÄŸer paketlerin baÄŸÄ±mlÄ±lÄ±k Ã§akÄ±ÅŸmasÄ±nÄ± gidermek iÃ§in:\n",
        "# 1. Ã–nce, Colab'in Ã¶nceden yÃ¼klediÄŸi ve Ã§akÄ±ÅŸmaya neden olabilecek tÃ¼m paketleri kaldÄ±rÄ±yoruz.\n",
        "!pip uninstall -y detectron2 fvcore torch torchvision torchaudio fastai\n",
        "# 2. ArdÄ±ndan, Colab'deki mevcut CUDA sÃ¼rÃ¼mÃ¼yle uyumlu PyTorch sÃ¼rÃ¼mÃ¼nÃ¼ kuruyoruz.\n",
        "!pip install torch torchvision torchaudio\n",
        "# 3. Son olarak, Detectron2'yi doÄŸrudan GitHub deposundan kuruyoruz.\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FASTER R CNN Ä°Ã‡Ä°N PAKET VE DOSYA KURULUMU\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n"
      ],
      "metadata": {
        "id": "CkLKXJ3S6AKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YENÄ° ANNATATÄ°ONS OLUÅžTURMA\n",
        "!pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from PIL import Image\n",
        "\n",
        "# --- Dosya YollarÄ± ve SÄ±nÄ±f Ä°simleri ---\n",
        "# Kendi dosya yollarÄ±nÄ±zÄ± burada gÃ¼ncelleyin\n",
        "model_path = \"#your file path/model_final.pth\"   # hazÄ±rlanan modelin drive kaydÄ±\n",
        "config_path = \"#your file path/config.yaml\"\n",
        "data_dir = \"/#your file path/train/\"\n",
        "test_annotations_path = os.path.join(data_dir, \"_annotations_fixed.coco.json\")\n",
        "output_annotations_path = os.path.join(data_dir, \"_annotations_fixed2.coco.json\")\n",
        "\n",
        "# Kendi sÄ±nÄ±f listenizi buraya girin (YAML dosyasÄ±ndan da Ã§ekilebilir, ancak bu daha kolay)\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# --- 1. Detectron2 Dataset'i Kaydetme ---\n",
        "try:\n",
        "    register_coco_instances(\"my_dataset_test\", {}, test_annotations_path, data_dir)\n",
        "    MetadataCatalog.get(\"my_dataset_test\").set(thing_classes=class_names)\n",
        "    print(\"Dataset registered successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error registering dataset: {e}\")\n",
        "\n",
        "# --- 2. Modeli ve KonfigÃ¼rasyonu YÃ¼kleme ---\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "\n",
        "cfg.MODEL.WEIGHTS = model_path\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2  # GÃ¼venilirlik eÅŸiÄŸi (trash_score) 0.3\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"Model loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# --- 3. Tahminleri AlÄ±p Yeni Annotation DosyasÄ± OluÅŸturma ---\n",
        "# Test veri setini Detectron2 formatÄ±nda yÃ¼kleyin\n",
        "test_dicts = DatasetCatalog.get(\"my_dataset_test\")\n",
        "\n",
        "new_coco_annotations = {\n",
        "    \"info\": {\"description\": \"Annotations generated by Detectron2 model with score threshold 0.3\"},\n",
        "    \"licenses\": [],\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": []\n",
        "}\n",
        "\n",
        "# Kategori listesini COCO formatÄ±na uygun ÅŸekilde oluÅŸturun\n",
        "for idx, name in enumerate(class_names):\n",
        "    new_coco_annotations['categories'].append({\"id\": idx + 1, \"name\": name, \"supercategory\": \"none\"})\n",
        "\n",
        "annotation_id_counter = 1\n",
        "\n",
        "# Tahminleri al ve yeni annotation'larÄ± oluÅŸtur\n",
        "for d in test_dicts:\n",
        "    img_path = d[\"file_name\"]\n",
        "    img_id = d[\"image_id\"]\n",
        "    height, width = d[\"height\"], d[\"width\"]\n",
        "\n",
        "    # GÃ¶rÃ¼ntÃ¼ dosyasÄ± yolunu tam olarak oluÅŸtur\n",
        "    full_img_path = os.path.join(data_dir, img_path)\n",
        "\n",
        "    if not os.path.exists(full_img_path):\n",
        "        print(f\"Warning: Image file not found at {full_img_path}. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing image: {img_path}\")\n",
        "\n",
        "    try:\n",
        "        # Predictor ile tahmin yapÄ±n\n",
        "        im = Image.open(full_img_path).convert(\"RGB\")\n",
        "        outputs = predictor(im)\n",
        "\n",
        "        instances = outputs[\"instances\"].to(\"cpu\")\n",
        "        boxes = instances.pred_boxes.tensor.numpy()\n",
        "        scores = instances.scores.numpy()\n",
        "        labels = instances.pred_classes.numpy()\n",
        "\n",
        "        # Resim bilgilerini ekleyin\n",
        "        new_coco_annotations[\"images\"].append({\n",
        "            \"id\": img_id,\n",
        "            \"file_name\": os.path.basename(full_img_path),\n",
        "            \"width\": width,\n",
        "            \"height\": height\n",
        "        })\n",
        "\n",
        "        # Tahminleri COCO formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            bbox = [float(x_min), float(y_min), float(x_max - x_min), float(y_max - y_min)]\n",
        "\n",
        "            # Detectron2'nin 0'dan baÅŸlayan label'Ä±na 1 ekleyerek COCO uyumlu category_id oluÅŸtur\n",
        "            coco_category_id = int(label) + 1\n",
        "\n",
        "            annotation = {\n",
        "                \"id\": annotation_id_counter,\n",
        "                \"image_id\": img_id,\n",
        "                \"category_id\": coco_category_id,\n",
        "                \"bbox\": bbox,\n",
        "                \"area\": float((x_max - x_min) * (y_max - y_min)),\n",
        "                \"iscrowd\": 0,\n",
        "                \"score\": float(score)\n",
        "            }\n",
        "            new_coco_annotations[\"annotations\"].append(annotation)\n",
        "            annotation_id_counter += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "# --- 4. Yeni Annotation DosyasÄ±nÄ± Kaydetme ---\n",
        "with open(output_annotations_path, 'w') as f:\n",
        "    json.dump(new_coco_annotations, f, indent=4)\n",
        "\n",
        "print(f\"\\nNew COCO annotations file created at: {output_annotations_path}\")"
      ],
      "metadata": {
        "id": "0WLiymN16UvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.model_zoo import get_config_file, get_checkpoint_url\n",
        "import os\n",
        "\n",
        "setup_logger()\n",
        "\n",
        "def register_my_dataset():\n",
        "    from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "    register_coco_instances(\n",
        "        \"tekstil_train_tek3\", {},\n",
        "        \"#your file path/train/_annotations.coco.json\",\n",
        "        \"#your file path/train\"\n",
        "    )\n",
        "    register_coco_instances(\n",
        "        \"tekstil_val_tek3\", {},\n",
        "        \"#your file path/valid/_annotations.coco.json\",\n",
        "        \"/#your file path/valid\"\n",
        "    )\n",
        "\n",
        "register_my_dataset()\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"tekstil_train_tek3\",)\n",
        "cfg.DATASETS.TEST = (\"tekstil_val_tek3\",)  # veya () boÅŸ olabilir\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Ã¶nceden eÄŸitilmiÅŸ aÄŸÄ±rlÄ±klar\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 5000  # ihtiyacÄ±na gÃ¶re arttÄ±r\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 8  # senin sÄ±nÄ±f sayÄ±n\n",
        "\n",
        "metadata = MetadataCatalog.get(\"tekstil_train\")\n",
        "metadata.thing_classes = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "model_path = os.path.join(cfg.OUTPUT_DIR, \"model3_final.pth\")\n",
        "trainer.checkpointer.save(\"model3_final\")\n",
        "print(f\"Model {model_path} olarak kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "4E_61vYQ-Y8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.model_zoo import get_config_file, get_checkpoint_url\n",
        "import os\n",
        "\n",
        "# Logger ayarla\n",
        "setup_logger()\n",
        "\n",
        "# 1. Datasetleri register et\n",
        "def register_datasets():\n",
        "    register_coco_instances(\n",
        "        \"tekstil_train_v3_teks33\", {},\n",
        "        \"/#your file path/train/_annotations.coco.json\",\n",
        "        \"/#your file path/train\"\n",
        "    )\n",
        "    register_coco_instances(\n",
        "        \"tekstil_val_v3_teks33\", {},\n",
        "        \"/#your file path/valid/_annotations.coco.json\",\n",
        "        \"/#your file path/valid\"\n",
        "    )\n",
        "\n",
        "register_datasets()\n",
        "\n",
        "# 2. SÄ±nÄ±f isimleri (metadata iÃ§in)\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# Metadata'ya sÄ±nÄ±flarÄ± ekle\n",
        "MetadataCatalog.get(\"tekstil_train_v3_teks33\").thing_classes = class_names\n",
        "MetadataCatalog.get(\"tekstil_val_v3_teks33\").thing_classes = class_names\n",
        "\n",
        "# 3. Config ayarla\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"tekstil_train_v3_teks33\",)\n",
        "cfg.DATASETS.TEST = (\"tekstil_val_v3_teks33\",)  # ya da ()\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Ã¶nceden eÄŸitilmiÅŸ aÄŸÄ±rlÄ±klar\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 5000  # ihtiyaca gÃ¶re artÄ±rabilirsin\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "\n",
        "# Ã‡Ä±ktÄ± dosyalarÄ±nÄ±n kaydedileceÄŸi klasÃ¶r\n",
        "cfg.OUTPUT_DIR = \"/#your file path/model_teks3_v2\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 4. Trainer oluÅŸtur ve eÄŸitimi baÅŸlat\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "# 5. EÄŸitim sonunda modeli kaydet\n",
        "model_path = os.path.join(cfg.OUTPUT_DIR, \"model3_final.pth\")\n",
        "trainer.checkpointer.save(\"model3_final\")\n",
        "print(f\"Model {model_path} olarak kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "Hq8fWVp0EDBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.model_zoo import get_config_file, get_checkpoint_url\n",
        "import os\n",
        "\n",
        "# Logger ayarla\n",
        "setup_logger()\n",
        "\n",
        "# 1. Datasetleri register et\n",
        "def register_datasets():\n",
        "    register_coco_instances(\n",
        "        \"tekstil_train_v5\", {},\n",
        "        \"/#your file path/train/_annotations.coco.json\",\n",
        "        \"/#your file path/train\"\n",
        "    )\n",
        "    register_coco_instances(\n",
        "        \"tekstil_val_v5\", {},\n",
        "        \"/#your file path/valid/_annotations.coco.json\",\n",
        "        \"/#your file path/valid\"\n",
        "    )\n",
        "\n",
        "register_datasets()\n",
        "\n",
        "# 2. SÄ±nÄ±f isimleri\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# Metadata'ya sÄ±nÄ±flarÄ± ekle\n",
        "MetadataCatalog.get(\"tekstil_train_v5\").thing_classes = class_names\n",
        "MetadataCatalog.get(\"tekstil_val_v5\").thing_classes = class_names\n",
        "\n",
        "# 3. Config ayarla\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"tekstil_train_v5\",)\n",
        "cfg.DATASETS.TEST = (\"tekstil_val_v5\",)  # istersen boÅŸ bÄ±rakabilirsin: ()\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Ã¶nceden eÄŸitilmiÅŸ aÄŸÄ±rlÄ±klar\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 8000  # ihtiyacÄ±na gÃ¶re artÄ±rabilirsin\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "\n",
        "# 4. Ã‡Ä±ktÄ± klasÃ¶rÃ¼\n",
        "cfg.OUTPUT_DIR = \"#your file path/model_teks_v5\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 5. Trainer oluÅŸtur ve eÄŸitimi baÅŸlat\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "# 6. Model aÄŸÄ±rlÄ±klarÄ±nÄ± kaydet\n",
        "model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "trainer.checkpointer.save(\"model_final\")\n",
        "print(f\"Model {model_path} olarak kaydedildi.\")\n",
        "\n",
        "# 7. Config dosyasÄ±nÄ± yaml olarak kaydet\n",
        "config_path = os.path.join(cfg.OUTPUT_DIR, \"config.yaml\")\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(cfg.dump())\n",
        "print(f\"Config dosyasÄ± {config_path} olarak kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "D_q6JtqOM6I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KESÄ°N  ANALÄ°Z\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# 1. SÄ±nÄ±f isimleri\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# 2. Metadata ayarÄ±\n",
        "dataset_name = \"tekstil_val_v5\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# 3. Config ve model yÃ¼kle\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"/#your file path/model_teks_v5/config.yaml\")\n",
        "cfg.MODEL.WEIGHTS = \"/#your file path/model_teks_v5/model_final.pth\"\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# 4. Test klasÃ¶rÃ¼ndeki gÃ¶rselleri analiz et\n",
        "test_folder = \"/#your file path/tekstil3test/train\"\n",
        "image_files = [f for f in os.listdir(test_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "for img_name in image_files:\n",
        "    img_path = os.path.join(test_folder, img_name)\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"YÃ¼klenemedi: {img_name}\")\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "    # GÃ¶rselleÅŸtir (renk kontrastÄ±nÄ± deÄŸiÅŸtirmeden)\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    img_result = out.get_image()[:, :, ::-1]  # BGR formatÄ±nda\n",
        "\n",
        "    # GÃ¶ster\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(cv2.cvtColor(img_result, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(img_name)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "GqpsPOsIHqgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# === 1. Paths ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "test_images_dir = \"/#your file path/train\"\n",
        "annotations_path = os.path.join(test_images_dir, \"_annotations_fixed2.coco.json\")\n",
        "output_dir = \"/#your file path/test_outputs_confusion_final2\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Class names ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ===\n",
        "dataset_name = \"tekstil_val_v5\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Config setup ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.25\n",
        "cfg.MODEL.DEVICE = \"cuda\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# === 5. Load annotations ===\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "\n",
        "filename_to_id = {img[\"file_name\"]: img[\"id\"] for img in coco_ann[\"images\"]}\n",
        "gt_map = {}\n",
        "for ann in coco_ann[\"annotations\"]:\n",
        "    image_id = ann[\"image_id\"]\n",
        "    cat_id = ann[\"category_id\"]\n",
        "    gt_map.setdefault(image_id, []).append(cat_id)\n",
        "\n",
        "# === 6. Prediction & comparison ===\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for file in tqdm(os.listdir(test_images_dir)):\n",
        "    if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        continue\n",
        "\n",
        "    image_path = os.path.join(test_images_dir, file)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(file)\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for gt_class in gt_classes:\n",
        "        if pred_classes:\n",
        "            y_true.append(gt_class)\n",
        "            y_pred.append(pred_classes[0])\n",
        "        else:\n",
        "            y_true.append(gt_class)\n",
        "            y_pred.append(-1)  # no prediction\n",
        "\n",
        "    # Save annotated image\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, file)\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 7. Confusion Matrix ===\n",
        "labels = list(range(len(class_names))) + [-1]\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# === 8. Classification Report ===\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX3MJb-VSkJM",
        "outputId": "961b56db-80a1-44d0-fdb4-70f043c9f73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129/129 [02:46<00:00,  1.29s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# === 1. Paths ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "test_images_dir = \"/#your file path/train\"\n",
        "# Orijinal ground truth (doÄŸru etiket) dosyasÄ±nÄ± kullanÄ±n\n",
        "annotations_path = os.path.join(test_images_dir, \"_annotations_fixed.coco.json\")\n",
        "output_dir = \"/#your file path/test_outputs_confusion_final4\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Class names ===\n",
        "# class_names listesi, Detectron2'nin label_id'sine (0, 1, 2...) karÅŸÄ±lÄ±k gelir\n",
        "# Fakat COCO'da label_id'ler genellikle 1'den baÅŸlar.\n",
        "# Kodun iÃ§indeki logic bu durumu dikkate alacaktÄ±r.\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ===\n",
        "dataset_name = \"tekstil_val_v5\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Config setup ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3\n",
        "cfg.MODEL.DEVICE = \"cuda\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# === 5. Load annotations ===\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "\n",
        "filename_to_id = {img[\"file_name\"]: img[\"id\"] for img in coco_ann[\"images\"]}\n",
        "gt_map = {}\n",
        "for ann in coco_ann[\"annotations\"]:\n",
        "    image_id = ann[\"image_id\"]\n",
        "    # COCO'daki etiketler 1'den baÅŸladÄ±ÄŸÄ± iÃ§in, 0'dan baÅŸlayan model etiketleriyle\n",
        "    # eÅŸleÅŸtirmek iÃ§in bir harita oluÅŸturulabilir veya 1 Ã§Ä±karÄ±labilir\n",
        "    cat_id = ann[\"category_id\"]\n",
        "    gt_map.setdefault(image_id, []).append(cat_id)\n",
        "\n",
        "# === 6. Prediction & comparison ===\n",
        "y_true, y_pred = [], []\n",
        "image_files = [f for f in os.listdir(test_images_dir) if f in filename_to_id]\n",
        "\n",
        "for file in tqdm(image_files):\n",
        "    image_path = os.path.join(test_images_dir, file)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "    image_id = filename_to_id[file]\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    # Tahminlerin ve ground truth'larÄ±n eÅŸleÅŸmesi iÃ§in basitleÅŸtirilmiÅŸ bir dÃ¶ngÃ¼\n",
        "    # Her bir ground truth iÃ§in bir tahmin eÅŸleÅŸtirilmeye Ã§alÄ±ÅŸÄ±lÄ±yor\n",
        "    for i, gt_class_id in enumerate(gt_classes):\n",
        "        # Ground truth etiketini y_true'ya ekle\n",
        "        y_true.append(gt_class_id)\n",
        "\n",
        "        # EÄŸer bir tahmin varsa, ilk tahmin edilen sÄ±nÄ±fÄ± kullan\n",
        "        if i < len(pred_classes):\n",
        "            # Model 0'dan baÅŸlayan etiketler Ã¼rettiÄŸi iÃ§in +1 ekliyoruz\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            # Tahmin yoksa, \"None\" olarak iÅŸaretle\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    # EÄŸer ground truth'tan daha fazla tahmin varsa, kalan tahminleri de ekle\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2) # Ekstra tahmin iÃ§in Ã¶zel bir etiket\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    # Save annotated image\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, file)\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 7. Confusion Matrix ===\n",
        "# Etiketleri, 1'den baÅŸlayan sÄ±nÄ±f ID'leri ve -1 (None) ile oluÅŸtur\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporu ve karmaÅŸÄ±klÄ±k matrisi iÃ§in gerekli etiketleri hazÄ±rlayÄ±n\n",
        "# EÄŸer y_true veya y_pred'de -2 etiketi varsa, bunu da dikkate alÄ±n\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# === 8. Classification Report ===\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "print(f\"Confusion Matrix and Classification Report saved to {output_dir}\")\n",
        "print(\"Script finished successfully.\")"
      ],
      "metadata": {
        "id": "iG0MEYeXKYMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import load_coco_json\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Ä°ki ayrÄ± test veri yolu ve annotation dosyasÄ±\n",
        "test_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(test_dir_1, \"_annotations_fixed2.coco.json\")\n",
        "\n",
        "test_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(test_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "output_dir = \"/#your file path/combined_test_outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. SÄ±nÄ±f Ä°simleri ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ve Config ===\n",
        "dataset_name = \"tekstil_combined_test\"\n",
        "# Toplam veri seti iÃ§in tek bir meta veri nesnesi oluÅŸturun\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# Modeli ve config'i yÃ¼kleyin\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"Model loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 4. BirleÅŸik Ground Truth Verilerini YÃ¼kleme ===\n",
        "# Her iki veri setinin etiketlerini ve gÃ¶rsel yollarÄ±nÄ± birleÅŸtirin\n",
        "combined_dicts = []\n",
        "combined_dicts.extend(load_coco_json(annotations_path_1, test_dir_1))\n",
        "combined_dicts.extend(load_coco_json(annotations_path_2, test_dir_2))\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "y_true, y_pred = [], []\n",
        "image_files_with_paths = []\n",
        "\n",
        "for d in combined_dicts:\n",
        "    # `d[\"file_name\"]` zaten tam yolu iÃ§eriyor\n",
        "    image_files_with_paths.append(d[\"file_name\"])\n",
        "\n",
        "for image_path in tqdm(image_files_with_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    # Ground truth sÄ±nÄ±flarÄ±nÄ± yÃ¼klenen verilerden alÄ±n\n",
        "    # COCO'da kategori ID'leri 1'den baÅŸlar, bu yÃ¼zden 1'den baÅŸlayan ID'ler y_true'ya eklenir\n",
        "    gt_classes = [ann['category_id'] for ann in d.get('annotations', []) if 'category_id' in ann]\n",
        "\n",
        "    # Tahminlerin ve ground truth'larÄ±n eÅŸleÅŸmesi iÃ§in basit dÃ¶ngÃ¼\n",
        "    # Her bir ground truth iÃ§in bir tahmin eÅŸleÅŸtirilir.\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "\n",
        "        if i < len(pred_classes):\n",
        "            # Model 0'dan baÅŸlayan etiketler Ã¼rettiÄŸi iÃ§in +1 ekliyoruz (COCO ID'si iÃ§in)\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1) # Tahmin yoksa, \"None\" olarak iÅŸaretle\n",
        "\n",
        "    # Fazla tahminleri (False Positives) ekle\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2) # Ekstra tahmin iÃ§in Ã¶zel bir etiket\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    # GÃ¶rseli kaydet\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    # Ã‡Ä±ktÄ± dosya adÄ±\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. KarmaÅŸÄ±klÄ±k Matrisi ve Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporu\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"Combined Confusion Matrix and Classification Report saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "XmB3_WQ2M4Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#baÅŸarÄ±sÄ±z confusion matrix hesabÄ± yaptÄ±\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "# === 1. Paths ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "test_images_dir = \"/#your file path/train\"\n",
        "annotations_path = os.path.join(test_images_dir, \"_annotations_fixed.coco.json\")\n",
        "output_dir = \"/#your file path/test_outputs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Class names ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ===\n",
        "dataset_name = \"tekstil_val_v5\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Config setup ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3\n",
        "cfg.MODEL.DEVICE = \"cuda\" if cv2.cuda.getCudaEnabledDeviceCount() > 0 else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# === 5. Load annotations ===\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "\n",
        "gt_map = {}\n",
        "for ann in coco_ann[\"annotations\"]:\n",
        "    image_id = ann[\"image_id\"]\n",
        "    cat_id = ann[\"category_id\"]\n",
        "    gt_map.setdefault(image_id, []).append(cat_id)\n",
        "\n",
        "filename_to_id = {img[\"file_name\"]: img[\"id\"] for img in coco_ann[\"images\"]}\n",
        "\n",
        "# === 6. Prediction & saving ===\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "for file in tqdm(os.listdir(test_images_dir)):\n",
        "    if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        continue\n",
        "\n",
        "    image_path = os.path.join(test_images_dir, file)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(file)\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for gt_class in gt_classes:\n",
        "        if pred_classes:\n",
        "            y_true.append(gt_class)\n",
        "            y_pred.append(pred_classes[0])\n",
        "        else:\n",
        "            y_true.append(gt_class)\n",
        "            y_pred.append(-1)  # No prediction\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, file)\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 7. Confusion Matrix ===\n",
        "labels = list(range(len(class_names)))\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels + [-1])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n"
      ],
      "metadata": {
        "id": "cQnYVxJuQsYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Ä°ki ayrÄ± test veri yolu\n",
        "test_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(test_dir_1, \"_annotations_fixed2.coco.json\")\n",
        "\n",
        "test_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(test_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "output_dir = \"/#your file path/combined_test_outputs33\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. SÄ±nÄ±f Ä°simleri ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ve Config ===\n",
        "dataset_name = \"tekstil_combined_test\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"Model loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 4. BirleÅŸik Ground Truth Verilerini YÃ¼kleme ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "def load_annotations_and_images(annotations_path, image_dir):\n",
        "    global gt_map, filename_to_id, all_image_paths\n",
        "\n",
        "    if not os.path.exists(annotations_path):\n",
        "        print(f\"Warning: Annotation file not found at {annotations_path}. Skipping this directory.\")\n",
        "        return\n",
        "\n",
        "    with open(annotations_path, \"r\") as f:\n",
        "        coco_ann = json.load(f)\n",
        "        for img in coco_ann[\"images\"]:\n",
        "            filename = img[\"file_name\"]\n",
        "            img_id = img[\"id\"]\n",
        "            full_path = os.path.join(image_dir, filename)\n",
        "\n",
        "            # EÄŸer aynÄ± ID'ye sahip bir dosya zaten iÅŸlenmiÅŸse, atla\n",
        "            if img_id in gt_map:\n",
        "                continue\n",
        "\n",
        "            filename_to_id[filename] = img_id\n",
        "            all_image_paths.append(full_path)\n",
        "        for ann in coco_ann[\"annotations\"]:\n",
        "            gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "load_annotations_and_images(annotations_path_1, test_dir_1)\n",
        "load_annotations_and_images(annotations_path_2, test_dir_2)\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    # Tahminlerin ve ground truth'larÄ±n eÅŸleÅŸmesi iÃ§in basitleÅŸtirilmiÅŸ dÃ¶ngÃ¼\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "\n",
        "        if i < len(pred_classes):\n",
        "            # Model 0'dan baÅŸlayan etiketler Ã¼rettiÄŸi iÃ§in +1 ekliyoruz (COCO ID'si iÃ§in)\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    # Fazla tahminleri (False Positives) de ekle\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    # GÃ¶rseli kaydet\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. KarmaÅŸÄ±klÄ±k Matrisi ve Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporu\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nCombined Confusion Matrix and Classification Report saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "9EYaHYjPRq03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Ä°ki ayrÄ± test veri yolu\n",
        "test_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(test_dir_1, \"_annotations_fixed2.coco.json\")\n",
        "\n",
        "test_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(test_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "output_dir = \"/#your file path/combined_test_outputs33\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. SÄ±nÄ±f Ä°simleri ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ve Config ===\n",
        "dataset_name = \"tekstil_combined_test\"\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"Model loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 4. BirleÅŸik Ground Truth Verilerini YÃ¼kleme ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "def load_annotations_and_images(annotations_path, image_dir):\n",
        "    global gt_map, filename_to_id, all_image_paths\n",
        "\n",
        "    if not os.path.exists(annotations_path):\n",
        "        print(f\"Warning: Annotation file not found at {annotations_path}. Skipping this directory.\")\n",
        "        return\n",
        "\n",
        "    with open(annotations_path, \"r\") as f:\n",
        "        coco_ann = json.load(f)\n",
        "        for img in coco_ann[\"images\"]:\n",
        "            filename = img[\"file_name\"]\n",
        "            img_id = img[\"id\"]\n",
        "            full_path = os.path.join(image_dir, filename)\n",
        "\n",
        "            # EÄŸer aynÄ± ID'ye sahip bir dosya zaten iÅŸlenmiÅŸse, atla\n",
        "            if img_id in gt_map:\n",
        "                continue\n",
        "\n",
        "            filename_to_id[filename] = img_id\n",
        "            all_image_paths.append(full_path)\n",
        "        for ann in coco_ann[\"annotations\"]:\n",
        "            gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "load_annotations_and_images(annotations_path_1, test_dir_1)\n",
        "load_annotations_and_images(annotations_path_2, test_dir_2)\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    # Tahminlerin ve ground truth'larÄ±n eÅŸleÅŸmesi iÃ§in basitleÅŸtirilmiÅŸ dÃ¶ngÃ¼\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    # Fazla tahminleri (False Positives) de ekle\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    # GÃ¶rseli kaydet\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. KarmaÅŸÄ±klÄ±k Matrisi ve Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporu\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nCombined Confusion Matrix and Classification Report saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "-bsv1m0SS0r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "# BirleÅŸtirilecek ilk veri seti klasÃ¶rÃ¼ ve annotation dosyasÄ±\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"_annotations_fixed.coco.json\")\n",
        "\n",
        "# BirleÅŸtirilecek ikinci veri seti klasÃ¶rÃ¼ ve annotation dosyasÄ±\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ verilerin kaydedileceÄŸi hedef klasÃ¶r\n",
        "combined_output_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_output_dir, \"combined_annotations.coco.json\")\n",
        "\n",
        "# Hedef klasÃ¶rÃ¼ oluÅŸtur (varsa silip yeniden oluÅŸturur)\n",
        "if os.path.exists(combined_output_dir):\n",
        "    shutil.rmtree(combined_output_dir)\n",
        "os.makedirs(combined_output_dir, exist_ok=True)\n",
        "\n",
        "# --- 2. BirleÅŸtirme Fonksiyonu ---\n",
        "def combine_coco_datasets(paths):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleÅŸtiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"UyarÄ±: Annotation dosyasÄ± bulunamadÄ±, atlanÄ±yor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Kategorileri harita kullanarak ekle veya gÃ¼ncelle\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        # Resim ID'lerini ve annotation ID'lerini gÃ¼ncelle ve kopyala\n",
        "        image_id_mapping = {}\n",
        "        for image in tqdm(data['images'], desc=f\"GÃ¶rseller kopyalanÄ±yor: {img_dir}\"):\n",
        "            old_image_id = image['id']\n",
        "            new_image_id = old_image_id + image_id_offset\n",
        "            image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "            image_copy = image.copy()\n",
        "            image_copy['id'] = new_image_id\n",
        "\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "            dst_image_path = os.path.join(combined_output_dir, image['file_name'])\n",
        "            if os.path.exists(src_image_path):\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "            combined_images.append(image_copy)\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            ann_copy = annotation.copy()\n",
        "            ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "            ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "            # Kategori ID'lerini yeni haritaya gÃ¶re gÃ¼ncelle\n",
        "            original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "            ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "            combined_annotations.append(ann_copy)\n",
        "\n",
        "        # Bir sonraki veri seti iÃ§in offset'leri gÃ¼ncelle\n",
        "        image_id_offset += len(data['images'])\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": data.get(\"licenses\", []),\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "# --- 3. BirleÅŸtirme Ä°ÅŸlemini BaÅŸlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "\n",
        "combined_dataset = combine_coco_datasets(paths_to_combine)\n",
        "\n",
        "with open(combined_annotations_path, \"w\") as f:\n",
        "    json.dump(combined_dataset, f, indent=4)\n",
        "\n",
        "print(f\"\\nBirleÅŸtirme tamamlandÄ±. GÃ¶rseller ve 'combined_annotations.coco.json' dosyasÄ± buraya kaydedildi: {combined_output_dir}\")"
      ],
      "metadata": {
        "id": "raWgK1a9Tg_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ veri setinin yolu\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "\n",
        "# Ã‡Ä±ktÄ±larÄ±n kaydedileceÄŸi klasÃ¶r\n",
        "output_dir = \"/#your file path/combined_analysis_report\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. SÄ±nÄ±f Ä°simleri ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "# === 3. Metadata ve Config ===\n",
        "dataset_name = \"tekstil_combined_dataset\"\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    # Veri seti zaten kayÄ±tlÄ±ysa hata vermesini engelle\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2  # Tahmin eÅŸiÄŸi\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"Model loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 4. Ground Truth Verilerini YÃ¼kleme ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    # Tahminlerin ve ground truth'larÄ±n eÅŸleÅŸmesi iÃ§in basitleÅŸtirilmiÅŸ dÃ¶ngÃ¼\n",
        "    # Not: Bu, her bir ground truth iÃ§in bir tahmin eÅŸleÅŸtirmeye Ã§alÄ±ÅŸÄ±r.\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "\n",
        "        if i < len(pred_classes):\n",
        "            # Model 0'dan baÅŸlayan etiketler Ã¼rettiÄŸi iÃ§in +1 ekliyoruz (COCO ID'si iÃ§in)\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1) # Tahmin yoksa, \"None\" olarak iÅŸaretle\n",
        "\n",
        "    # Fazla tahminleri (False Positives) de ekle\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2) # Ekstra tahmin iÃ§in Ã¶zel bir etiket\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    # GÃ¶rseli tahminlerle kaydet\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. KarmaÅŸÄ±klÄ±k Matrisi ve Raporu ===\n",
        "# Etiketleri, 1'den baÅŸlayan sÄ±nÄ±f ID'leri ve -1 (None) ile oluÅŸtur\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporu\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandÄ±. Raporlar ve gÃ¶rseller buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "LyKiBQ1LTy8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yollarÄ±\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"_annotations_fixed2.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ verilerin ve Ã§Ä±ktÄ±larÄ±n kaydedileceÄŸi ana klasÃ¶rler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report3\"\n",
        "\n",
        "# KlasÃ¶rleri oluÅŸtur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri KÃ¼melerini BirleÅŸtirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleÅŸtiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"UyarÄ±: Annotation dosyasÄ± bulunamadÄ±, atlanÄ±yor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        for image in tqdm(data['images'], desc=f\"GÃ¶rseller kopyalanÄ±yor: {img_dir}\"):\n",
        "            old_image_id = image['id']\n",
        "            new_image_id = old_image_id + image_id_offset\n",
        "            image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "            image_copy = image.copy()\n",
        "            image_copy['id'] = new_image_id\n",
        "\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "            dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "            if os.path.exists(src_image_path):\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "            combined_images.append(image_copy)\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            ann_copy = annotation.copy()\n",
        "            ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "            ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "            original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "            ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "            combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += len(data['images'])\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleÅŸtirme tamamlandÄ±.\")\n",
        "\n",
        "# --- BirleÅŸtirme Ä°ÅŸlemini BaÅŸlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz iÃ§in SÄ±nÄ±f Ä°simleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\",\n",
        "    \"1- patlak\",\n",
        "    \"2- igne_kirigi\",\n",
        "    \"3- jut\",\n",
        "    \"5- likra_kacigi\",\n",
        "    \"6- yag_lekesi\",\n",
        "    \"8- May cizgisi\",\n",
        "    \"fsa\"\n",
        "]\n",
        "\n",
        "dataset_name = \"tekstil_combined_dataset\"\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model YÃ¼kleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. KarmaÅŸÄ±klÄ±k Matrisi ve Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandÄ±. Raporlar ve gÃ¶rseller buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "0TCsbpbjUdqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yollarÄ±\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ verilerin ve Ã§Ä±ktÄ±larÄ±n kaydedileceÄŸi ana klasÃ¶rler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report22\"\n",
        "\n",
        "# KlasÃ¶rleri oluÅŸtur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri KÃ¼melerini BirleÅŸtirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleÅŸtiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"UyarÄ±: Annotation dosyasÄ± bulunamadÄ±, atlanÄ±yor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"GÃ¶rseller kopyalanÄ±yor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarÄ±: {src_image_path} bulunamadÄ±, bu gÃ¶rsel atlanÄ±yor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasÃ¶rÃ¼nden {processed_count} gÃ¶rsel baÅŸarÄ±yla kopyalandÄ±, {skipped_count} gÃ¶rsel atlandÄ±.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleÅŸtirme tamamlandÄ±.\")\n",
        "\n",
        "# --- BirleÅŸtirme Ä°ÅŸlemini BaÅŸlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz iÃ§in SÄ±nÄ±f Ä°simleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model YÃ¼kleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. KarmaÅŸÄ±klÄ±k Matrisi ve Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandÄ±. Raporlar ve gÃ¶rseller buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "p3uRIh_4VGKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama (Colab iÃ§in)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yollarÄ±\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ verilerin ve Ã§Ä±ktÄ±larÄ±n kaydedileceÄŸi ana klasÃ¶rler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report33\"\n",
        "\n",
        "# KlasÃ¶rleri oluÅŸtur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri KÃ¼melerini BirleÅŸtirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleÅŸtiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"UyarÄ±: Annotation dosyasÄ± bulunamadÄ±, atlanÄ±yor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"GÃ¶rseller kopyalanÄ±yor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarÄ±: {src_image_path} bulunamadÄ±, bu gÃ¶rsel atlanÄ±yor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasÃ¶rÃ¼nden {processed_count} gÃ¶rsel baÅŸarÄ±yla kopyalandÄ±, {skipped_count} gÃ¶rsel atlandÄ±.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleÅŸtirme tamamlandÄ±.\")\n",
        "\n",
        "# --- BirleÅŸtirme Ä°ÅŸlemini BaÅŸlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "# === 3. Analiz iÃ§in SÄ±nÄ±f Ä°simleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model YÃ¼kleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. KarmaÅŸÄ±klÄ±k Matrisi ve DetaylÄ± Metrik Raporu ===\n",
        "# Etiketleri, 1'den baÅŸlayan sÄ±nÄ±f ID'leri ve -1 (None) ile oluÅŸtur\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "# KarmaÅŸÄ±klÄ±k Matrisi gÃ¶rselleÅŸtirmesini kaydet\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# DetaylÄ± TP, TN, FP, FN deÄŸerlerini hesaplayÄ±p raporla\n",
        "total_samples = len(y_true)\n",
        "report_content = f\"### DetaylÄ± Metrik Raporu\\n\\nToplam Analiz Edilen Ã–rnek SayÄ±sÄ±: {total_samples}\\n\\n\"\n",
        "\n",
        "for i, class_label in enumerate(disp_labels):\n",
        "    tp = cm[i, i]\n",
        "    fp = np.sum(cm[:, i]) - tp\n",
        "    fn = np.sum(cm[i, :]) - tp\n",
        "    tn = np.sum(cm) - (tp + fp + fn)\n",
        "\n",
        "    tp_percent = (tp / total_samples) * 100 if total_samples > 0 else 0\n",
        "    tn_percent = (tn / total_samples) * 100 if total_samples > 0 else 0\n",
        "    fp_percent = (fp / total_samples) * 100 if total_samples > 0 else 0\n",
        "    fn_percent = (fn / total_samples) * 100 if total_samples > 0 else 0\n",
        "\n",
        "    report_content += f\"--- {class_label} ---\\n\"\n",
        "    report_content += f\"TP (DoÄŸru Pozitif): {tp} ({tp_percent:.2f}%)\\n\"\n",
        "    report_content += f\"TN (DoÄŸru Negatif): {tn} ({tn_percent:.2f}%)\\n\"\n",
        "    report_content += f\"FP (YanlÄ±ÅŸ Pozitif): {fp} ({fp_percent:.2f}%)\\n\"\n",
        "    report_content += f\"FN (YanlÄ±ÅŸ Negatif): {fn} ({fn_percent:.2f}%)\\n\\n\"\n",
        "\n",
        "# Raporu dosyaya kaydet\n",
        "detailed_report_path = os.path.join(output_dir, \"detailed_metrics_report.txt\")\n",
        "with open(detailed_report_path, \"w\") as f:\n",
        "    f.write(report_content)\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandÄ±. Raporlar ve gÃ¶rseller buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "hgvMEyZdYWa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama (Colab iÃ§in)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yollarÄ±\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ verilerin ve Ã§Ä±ktÄ±larÄ±n kaydedileceÄŸi ana klasÃ¶rler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report22\"\n",
        "\n",
        "# KlasÃ¶rleri oluÅŸtur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri KÃ¼melerini BirleÅŸtirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleÅŸtiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"UyarÄ±: Annotation dosyasÄ± bulunamadÄ±, atlanÄ±yor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"GÃ¶rseller kopyalanÄ±yor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarÄ±: {src_image_path} bulunamadÄ±, bu gÃ¶rsel atlanÄ±yor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasÃ¶rÃ¼nden {processed_count} gÃ¶rsel baÅŸarÄ±yla kopyalandÄ±, {skipped_count} gÃ¶rsel atlandÄ±.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleÅŸtirme tamamlandÄ±.\")\n",
        "\n",
        "# --- BirleÅŸtirme Ä°ÅŸlemini BaÅŸlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz iÃ§in SÄ±nÄ±f Ä°simleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model YÃ¼kleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. KarmaÅŸÄ±klÄ±k Matrisi ve DetaylÄ± Metrik Raporu ===\n",
        "# Etiketleri, 1'den baÅŸlayan sÄ±nÄ±f ID'leri ve -1 (None) ile oluÅŸtur\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "# KarmaÅŸÄ±klÄ±k Matrisi gÃ¶rselleÅŸtirmesini kaydet\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# DetaylÄ± TP, TN, FP, FN deÄŸerlerini hesaplayÄ±p gÃ¶rselleÅŸtirme\n",
        "total_samples = len(y_true)\n",
        "num_classes = len(disp_labels)\n",
        "metrics_data = np.zeros((num_classes, 4), dtype=int)\n",
        "metrics_percentage_data = np.zeros((num_classes, 4), dtype=float)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    tp = cm[i, i]\n",
        "    fp = np.sum(cm[:, i]) - tp\n",
        "    fn = np.sum(cm[i, :]) - tp\n",
        "    tn = np.sum(cm) - (tp + fp + fn)\n",
        "\n",
        "    metrics_data[i, 0] = tp\n",
        "    metrics_data[i, 1] = fp\n",
        "    metrics_data[i, 2] = fn\n",
        "    metrics_data[i, 3] = tn\n",
        "\n",
        "    tp_perc = (tp / total_samples) * 100 if total_samples > 0 else 0\n",
        "    fp_perc = (fp / total_samples) * 100 if total_samples > 0 else 0\n",
        "    fn_perc = (fn / total_samples) * 100 if total_samples > 0 else 0\n",
        "    tn_perc = (tn / total_samples) * 100 if total_samples > 0 else 0\n",
        "\n",
        "    metrics_percentage_data[i, 0] = tp_perc\n",
        "    metrics_percentage_data[i, 1] = fp_perc\n",
        "    metrics_percentage_data[i, 2] = fn_perc\n",
        "    metrics_percentage_data[i, 3] = tn_perc\n",
        "\n",
        "# Matrisi Ã§izme\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "columns = ['TP (DoÄŸru Pozitif)', 'FP (YanlÄ±ÅŸ Pozitif)', 'FN (YanlÄ±ÅŸ Negatif)', 'TN (DoÄŸru Negatif)']\n",
        "ax.set_title(\"DetaylÄ± Metrik Matrisi\")\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "cell_text = []\n",
        "for i in range(num_classes):\n",
        "    row_text = []\n",
        "    for j in range(4):\n",
        "        value = metrics_data[i, j]\n",
        "        percentage = metrics_percentage_data[i, j]\n",
        "        row_text.append(f\"{value}\\n({percentage:.2f}%)\")\n",
        "    cell_text.append(row_text)\n",
        "\n",
        "table = ax.table(cellText=cell_text, colLabels=columns, rowLabels=disp_labels, loc='center', cellLoc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1.2, 1.2)\n",
        "\n",
        "for (i, j), cell in table.get_celld().items():\n",
        "    if i == 0:  # BaÅŸlÄ±k satÄ±rÄ±\n",
        "        cell.set_facecolor(\"#4CAF50\")\n",
        "    else:\n",
        "        # TP ve TN iÃ§in yeÅŸil, FP ve FN iÃ§in kÄ±rmÄ±zÄ±\n",
        "        if j == 0 or j == 3:\n",
        "            cell.set_facecolor(\"#E8F5E9\") # AÃ§Ä±k yeÅŸil\n",
        "        else:\n",
        "            cell.set_facecolor(\"#FFEBEE\") # AÃ§Ä±k kÄ±rmÄ±zÄ±\n",
        "\n",
        "plt.tight_layout()\n",
        "metrics_matrix_path = os.path.join(output_dir, \"detailed_metrics_matrix.png\")\n",
        "plt.savefig(metrics_matrix_path, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.close()\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandÄ±. Raporlar ve gÃ¶rseller buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "ROJp2LXWb5pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama (Colab iÃ§in)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yollarÄ±\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ verilerin ve Ã§Ä±ktÄ±larÄ±n kaydedileceÄŸi ana klasÃ¶rler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report55\"\n",
        "\n",
        "# KlasÃ¶rleri oluÅŸtur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri KÃ¼melerini BirleÅŸtirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleÅŸtiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"UyarÄ±: Annotation dosyasÄ± bulunamadÄ±, atlanÄ±yor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"GÃ¶rseller kopyalanÄ±yor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarÄ±: {src_image_path} bulunamadÄ±, bu gÃ¶rsel atlanÄ±yor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasÃ¶rÃ¼nden {processed_count} gÃ¶rsel baÅŸarÄ±yla kopyalandÄ±, {skipped_count} gÃ¶rsel atlandÄ±.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleÅŸtirme tamamlandÄ±.\")\n",
        "\n",
        "# --- BirleÅŸtirme Ä°ÅŸlemini BaÅŸlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz iÃ§in SÄ±nÄ±f Ä°simleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model YÃ¼kleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. KarmaÅŸÄ±klÄ±k Matrisi ve DetaylÄ± Metrik Raporu ===\n",
        "labels = list(range(1, len(class_names) + 1)) + [-1]\n",
        "disp_labels = class_names + [\"None\"]\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "# KarmaÅŸÄ±klÄ±k Matrisi gÃ¶rselleÅŸtirmesini kaydet\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
        "disp.plot(include_values=True, xticks_rotation=45, ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"Combined Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"combined_confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.close()\n",
        "\n",
        "# DetaylÄ± TP, TN, FP, FN deÄŸerlerini hesaplayÄ±p gÃ¶rselleÅŸtirme\n",
        "total_samples = len(y_true)\n",
        "num_classes = len(disp_labels)\n",
        "\n",
        "metrics_data = np.zeros((num_classes, 4), dtype=int)\n",
        "metrics_percentage_data = np.zeros((num_classes, 4), dtype=float)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    tp = cm[i, i]\n",
        "    fp = np.sum(cm[:, i]) - tp\n",
        "    fn = np.sum(cm[i, :]) - tp\n",
        "    tn = np.sum(cm) - (tp + fp + fn)\n",
        "\n",
        "    metrics_data[i, 0] = tp\n",
        "    metrics_data[i, 1] = fp\n",
        "    metrics_data[i, 2] = fn\n",
        "    metrics_data"
      ],
      "metadata": {
        "id": "Ie40_BuMdHYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama (Colab iÃ§in)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yollarÄ±\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ verilerin ve Ã§Ä±ktÄ±larÄ±n kaydedileceÄŸi ana klasÃ¶rler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report66\"\n",
        "\n",
        "# KlasÃ¶rleri oluÅŸtur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri KÃ¼melerini BirleÅŸtirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleÅŸtiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"UyarÄ±: Annotation dosyasÄ± bulunamadÄ±, atlanÄ±yor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"GÃ¶rseller kopyalanÄ±yor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarÄ±: {src_image_path} bulunamadÄ±, bu gÃ¶rsel atlanÄ±yor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasÃ¶rÃ¼nden {processed_count} gÃ¶rsel baÅŸarÄ±yla kopyalandÄ±, {skipped_count} gÃ¶rsel atlandÄ±.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleÅŸtirme tamamlandÄ±.\")\n",
        "\n",
        "# --- BirleÅŸtirme Ä°ÅŸlemini BaÅŸlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz iÃ§in SÄ±nÄ±f Ä°simleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model YÃ¼kleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Genel BaÅŸarÄ± Matrisi (TP/FP/FN/TN) OluÅŸturma ===\n",
        "# TÃ¼m defect sÄ±nÄ±flarÄ±nÄ± \"HatalÄ±\" olarak, None ve DiÄŸer sÄ±nÄ±flarÄ± \"HatasÄ±z\" olarak kabul etme\n",
        "defect_class_ids = set(range(1, len(class_names) + 1))\n",
        "y_true_binary = [1 if val in defect_class_ids else 0 for val in y_true]\n",
        "y_pred_binary = [1 if val in defect_class_ids else 0 for val in y_pred]\n",
        "\n",
        "# 2x2 karÄ±ÅŸÄ±klÄ±k matrisini hesapla\n",
        "cm = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "total_samples = tn + fp + fn + tp\n",
        "\n",
        "# Matrisi istenen dÃ¼zende gÃ¶rselleÅŸtirme\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.axis('off')\n",
        "ax.set_title('Genel BaÅŸarÄ± Matrisi', fontsize=18, pad=20)\n",
        "\n",
        "# Verileri ve etiketleri hazÄ±rlama\n",
        "cell_texts = np.array([\n",
        "    [f\"TP: {tp}\\n({tp/total_samples*100:.2f}%)\", f\"FP: {fp}\\n({fp/total_samples*100:.2f}%)\"],\n",
        "    [f\"FN: {fn}\\n({fn/total_samples*100:.2f}%)\", f\"TN: {tn}\\n({tn/total_samples*100:.2f}%)\"]\n",
        "])\n",
        "row_labels = [\"GerÃ§ek HatalÄ± (Expected)\", \"GerÃ§ek HatasÄ±z\"]\n",
        "col_labels = [\"Tahmin Edilen HatalÄ± (Predicted)\", \"Tahmin Edilen HatasÄ±z\"]\n",
        "\n",
        "# Matrisi Ã§izme\n",
        "table = ax.table(cellText=cell_texts,\n",
        "                 rowLabels=row_labels,\n",
        "                 colLabels=col_labels,\n",
        "                 loc='center',\n",
        "                 cellLoc='center')\n",
        "\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "table.scale(1.2, 1.2)\n",
        "\n",
        "# Renklendirme\n",
        "table.get_celld()[1, 0].set_facecolor('#d9f1e1')  # TP\n",
        "table.get_celld()[1, 1].set_facecolor('#f8e2e2')  # FP\n",
        "table.get_celld()[0, 0].set_facecolor('#f8e2e2')  # FN\n",
        "table.get_celld()[0, 1].set_facecolor('#d9f1e1')  # TN\n",
        "\n",
        "plt.tight_layout()\n",
        "matrix_path = os.path.join(output_dir, \"genel_basari_matrisi.png\")\n",
        "plt.savefig(matrix_path, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.close()\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandÄ±. Genel BaÅŸarÄ± Matrisi buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "tkMrS51GfLLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama (Colab iÃ§in)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yollarÄ±\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ verilerin ve Ã§Ä±ktÄ±larÄ±n kaydedileceÄŸi ana klasÃ¶rler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report666\"\n",
        "\n",
        "# KlasÃ¶rleri oluÅŸtur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri KÃ¼melerini BirleÅŸtirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleÅŸtiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"UyarÄ±: Annotation dosyasÄ± bulunamadÄ±, atlanÄ±yor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"GÃ¶rseller kopyalanÄ±yor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarÄ±: {src_image_path} bulunamadÄ±, bu gÃ¶rsel atlanÄ±yor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasÃ¶rÃ¼nden {processed_count} gÃ¶rsel baÅŸarÄ±yla kopyalandÄ±, {skipped_count} gÃ¶rsel atlandÄ±.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleÅŸtirme tamamlandÄ±.\")\n",
        "\n",
        "# --- BirleÅŸtirme Ä°ÅŸlemini BaÅŸlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz iÃ§in SÄ±nÄ±f Ä°simleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model YÃ¼kleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Genel BaÅŸarÄ± Matrisi (TP/FP/FN/TN) OluÅŸturma ===\n",
        "# TÃ¼m defect sÄ±nÄ±flarÄ±nÄ± \"HatalÄ±\" olarak, None ve DiÄŸer sÄ±nÄ±flarÄ± \"HatasÄ±z\" olarak kabul etme\n",
        "defect_class_ids = set(range(1, len(class_names) + 1))\n",
        "y_true_binary = [1 if val in defect_class_ids else 0 for val in y_true]\n",
        "y_pred_binary = [1 if val in defect_class_ids else 0 for val in y_pred]\n",
        "\n",
        "# 2x2 karÄ±ÅŸÄ±klÄ±k matrisini hesapla\n",
        "cm = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "total_samples = tn + fp + fn + tp\n",
        "\n",
        "# Matrisi istenen dÃ¼zende gÃ¶rselleÅŸtirme\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.set_title('Genel BaÅŸarÄ± Matrisi', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Tahmin Edilen', fontsize=14)\n",
        "ax.set_ylabel('GerÃ§ek', fontsize=14)\n",
        "\n",
        "# Renk haritasÄ± iÃ§in veri\n",
        "heatmap_data = np.array([[tp, fp], [fn, tn]])\n",
        "normalized_data = heatmap_data / np.max(heatmap_data) if np.max(heatmap_data) > 0 else np.zeros((2,2))\n",
        "\n",
        "# Renklendirme\n",
        "cmap = plt.cm.get_cmap('Greens')\n",
        "cmap_r = plt.cm.get_cmap('Reds')\n",
        "\n",
        "# TP ve TN iÃ§in yeÅŸil tonlarÄ±, FP ve FN iÃ§in kÄ±rmÄ±zÄ± tonlarÄ±\n",
        "colors = np.zeros((2, 2, 4))\n",
        "colors[0, 0] = cmap(normalized_data[0, 0]) # TP\n",
        "colors[0, 1] = cmap_r(normalized_data[0, 1]) # FP\n",
        "colors[1, 0] = cmap_r(normalized_data[1, 0]) # FN\n",
        "colors[1, 1] = cmap(normalized_data[1, 1]) # TN\n",
        "\n",
        "# Matrisin kendisi\n",
        "ax.imshow(colors, interpolation='nearest', origin='upper')\n",
        "\n",
        "# Metinleri hÃ¼crelere yazma\n",
        "labels_text = [\n",
        "    [f\"TP\\n{tp}\\n({tp/total_samples*100:.2f}%)\", f\"FP\\n{fp}\\n({fp/total_samples*100:.2f}%)\"],\n",
        "    [f\"FN\\n{fn}\\n({fn/total_samples*100:.2f}%)\", f\"TN\\n{tn}\\n({tn/total_samples*100:.2f}%)\"]\n",
        "]\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, labels_text[i][j], ha='center', va='center', color='black', fontsize=16, weight='bold')\n",
        "\n",
        "# Eksen etiketlerini ve iÅŸaretlerini ayarla\n",
        "ax.set_xticks(np.arange(2))\n",
        "ax.set_yticks(np.arange(2))\n",
        "ax.set_xticklabels([\"HatalÄ±\", \"HatasÄ±z\"])\n",
        "ax.set_yticklabels([\"HatalÄ±\", \"HatasÄ±z\"])\n",
        "\n",
        "plt.tight_layout()\n",
        "matrix_path = os.path.join(output_dir, \"genel_basari_matrisi.png\")\n",
        "plt.savefig(matrix_path, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.close()\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true, y_pred, labels=labels,\n",
        "    target_names=disp_labels,\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"combined_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandÄ±. Genel BaÅŸarÄ± Matrisi buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "tRjqs5-bgK8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama (Colab iÃ§in)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yollarÄ±\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ verilerin ve Ã§Ä±ktÄ±larÄ±n kaydedileceÄŸi ana klasÃ¶rler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report6666\"\n",
        "\n",
        "# KlasÃ¶rleri oluÅŸtur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri KÃ¼melerini BirleÅŸtirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleÅŸtiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"UyarÄ±: Annotation dosyasÄ± bulunamadÄ±, atlanÄ±yor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"GÃ¶rseller kopyalanÄ±yor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarÄ±: {src_image_path} bulunamadÄ±, bu gÃ¶rsel atlanÄ±yor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasÃ¶rÃ¼nden {processed_count} gÃ¶rsel baÅŸarÄ±yla kopyalandÄ±, {skipped_count} gÃ¶rsel atlandÄ±.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleÅŸtirme tamamlandÄ±.\")\n",
        "\n",
        "# --- BirleÅŸtirme Ä°ÅŸlemini BaÅŸlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz iÃ§in SÄ±nÄ±f Ä°simleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model YÃ¼kleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# === 6. Genel BaÅŸarÄ± Matrisi (TP/FP/FN/TN) OluÅŸturma ===\n",
        "\n",
        "# TÃ¼m defect sÄ±nÄ±flarÄ±nÄ± \"HatalÄ±\" olarak, None ve DiÄŸer sÄ±nÄ±flarÄ± \"HatasÄ±z\" olarak kabul etme\n",
        "defect_class_ids = set(range(1, len(class_names) + 1))\n",
        "y_true_binary = [1 if val in defect_class_ids else 0 for val in y_true]\n",
        "y_pred_binary = [1 if val in defect_class_ids else 0 for val in y_pred]\n",
        "\n",
        "# 2x2 karÄ±ÅŸÄ±klÄ±k matrisini hesapla\n",
        "cm = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "total_samples = tn + fp + fn + tp\n",
        "\n",
        "# Matrisi istenen dÃ¼zende gÃ¶rselleÅŸtirme\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.set_title('Genel BaÅŸarÄ± Matrisi', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Tahmin Edilen', fontsize=14)\n",
        "ax.set_ylabel('GerÃ§ek', fontsize=14)\n",
        "\n",
        "# Renk haritasÄ± iÃ§in veri\n",
        "heatmap_data = np.array([[tp, fp], [fn, tn]])\n",
        "normalized_data = heatmap_data / np.max(heatmap_data) if np.max(heatmap_data) > 0 else np.zeros((2,2))\n",
        "\n",
        "# Renklendirme\n",
        "cmap_green = plt.cm.get_cmap('Greens')\n",
        "cmap_red = plt.cm.get_cmap('Reds')\n",
        "\n",
        "# TP ve TN iÃ§in yeÅŸil tonlarÄ±, FP ve FN iÃ§in kÄ±rmÄ±zÄ± tonlarÄ±\n",
        "colors = np.zeros((2, 2, 4))\n",
        "colors[0, 0] = cmap_green(normalized_data[0, 0]) # TP\n",
        "colors[0, 1] = cmap_red(normalized_data[0, 1])   # FP\n",
        "colors[1, 0] = cmap_red(normalized_data[1, 0])   # FN\n",
        "colors[1, 1] = cmap_green(normalized_data[1, 1]) # TN\n",
        "\n",
        "# Matrisin kendisi\n",
        "ax.imshow(colors, interpolation='nearest', origin='upper')\n",
        "\n",
        "# Metinleri hÃ¼crelere yazma\n",
        "labels_text = [\n",
        "    [f\"TP\\n{tp}\\n({tp/total_samples*100:.2f}%)\", f\"FP\\n{fp}\\n({fp/total_samples*100:.2f}%)\"],\n",
        "    [f\"FN\\n{fn}\\n({fn/total_samples*100:.2f}%)\", f\"TN\\n{tn}\\n({tn/total_samples*100:.2f}%)\"]\n",
        "]\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, labels_text[i][j], ha='center', va='center', color='black', fontsize=16, weight='bold')\n",
        "\n",
        "# Eksen etiketlerini ve iÅŸaretlerini ayarla\n",
        "ax.set_xticks(np.arange(2))\n",
        "ax.set_yticks(np.arange(2))\n",
        "ax.set_xticklabels([\"HatalÄ±\", \"HatasÄ±z\"])\n",
        "ax.set_yticklabels([\"HatalÄ±\", \"HatasÄ±z\"])\n",
        "\n",
        "plt.tight_layout()\n",
        "matrix_path = os.path.join(output_dir, \"genel_basari_matrisi.png\")\n",
        "plt.savefig(matrix_path, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.close()\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true_binary, y_pred_binary, labels=[0, 1],\n",
        "    target_names=[\"HatasÄ±z\", \"HatalÄ±\"],\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"binary_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandÄ±. Genel BaÅŸarÄ± Matrisi buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "5jZnta0ChocP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama (Colab iÃ§in)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === 1. Dosya YollarÄ± ===\n",
        "config_path = \"/#your file path/config.yaml\"\n",
        "weights_path = \"/#your file path/model_final.pth\"\n",
        "\n",
        "# Kaynak veri setlerinin yollarÄ±\n",
        "source_dir_1 = \"/#your file path/train\"\n",
        "annotations_path_1 = os.path.join(source_dir_1, \"ikinci.coco.json\")\n",
        "source_dir_2 = \"/#your file path/test\"\n",
        "annotations_path_2 = os.path.join(source_dir_2, \"_annotations.coco.json\")\n",
        "\n",
        "# BirleÅŸtirilmiÅŸ verilerin ve Ã§Ä±ktÄ±larÄ±n kaydedileceÄŸi ana klasÃ¶rler\n",
        "combined_dataset_dir = \"/#your file path/combined_dataset\"\n",
        "combined_annotations_path = os.path.join(combined_dataset_dir, \"combined_annotations.coco.json\")\n",
        "output_dir = \"/#your file path/combined_analysis_report66666\"\n",
        "\n",
        "# KlasÃ¶rleri oluÅŸtur/temizle\n",
        "if os.path.exists(combined_dataset_dir):\n",
        "    shutil.rmtree(combined_dataset_dir)\n",
        "os.makedirs(combined_dataset_dir, exist_ok=True)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# === 2. Veri KÃ¼melerini BirleÅŸtirme Fonksiyonu ===\n",
        "def combine_coco_datasets(paths, output_ann_path, output_img_dir):\n",
        "    combined_images = []\n",
        "    combined_annotations = []\n",
        "    combined_categories = []\n",
        "\n",
        "    image_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "    category_id_map = {}\n",
        "\n",
        "    print(\"Veri setleri birleÅŸtiriliyor...\")\n",
        "\n",
        "    for ann_path, img_dir in paths:\n",
        "        if not os.path.exists(ann_path):\n",
        "            print(f\"UyarÄ±: Annotation dosyasÄ± bulunamadÄ±, atlanÄ±yor: {ann_path}\")\n",
        "            continue\n",
        "\n",
        "        with open(ann_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for cat in data['categories']:\n",
        "            if cat['name'] not in category_id_map:\n",
        "                new_id = len(category_id_map) + 1\n",
        "                category_id_map[cat['name']] = new_id\n",
        "                combined_categories.append({'id': new_id, 'name': cat['name'], 'supercategory': cat.get('supercategory', 'none')})\n",
        "\n",
        "        image_id_mapping = {}\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        for image in tqdm(data['images'], desc=f\"GÃ¶rseller kopyalanÄ±yor: {img_dir}\"):\n",
        "            src_image_path = os.path.join(img_dir, image['file_name'])\n",
        "\n",
        "            if os.path.exists(src_image_path):\n",
        "                old_image_id = image['id']\n",
        "                new_image_id = old_image_id + image_id_offset\n",
        "                image_id_mapping[old_image_id] = new_image_id\n",
        "\n",
        "                image_copy = image.copy()\n",
        "                image_copy['id'] = new_image_id\n",
        "\n",
        "                dst_image_path = os.path.join(output_img_dir, image['file_name'])\n",
        "                shutil.copy(src_image_path, dst_image_path)\n",
        "\n",
        "                combined_images.append(image_copy)\n",
        "                processed_count += 1\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                print(f\"\\nUyarÄ±: {src_image_path} bulunamadÄ±, bu gÃ¶rsel atlanÄ±yor.\")\n",
        "\n",
        "        print(f\"\\n{img_dir} klasÃ¶rÃ¼nden {processed_count} gÃ¶rsel baÅŸarÄ±yla kopyalandÄ±, {skipped_count} gÃ¶rsel atlandÄ±.\")\n",
        "\n",
        "        for annotation in data['annotations']:\n",
        "            if annotation['image_id'] in image_id_mapping:\n",
        "                ann_copy = annotation.copy()\n",
        "                ann_copy['id'] = annotation['id'] + ann_id_offset\n",
        "                ann_copy['image_id'] = image_id_mapping[annotation['image_id']]\n",
        "\n",
        "                original_cat_name = next(cat['name'] for cat in data['categories'] if cat['id'] == annotation['category_id'])\n",
        "                ann_copy['category_id'] = category_id_map[original_cat_name]\n",
        "\n",
        "                combined_annotations.append(ann_copy)\n",
        "\n",
        "        image_id_offset += processed_count\n",
        "        ann_id_offset += len(data['annotations'])\n",
        "\n",
        "    combined_data = {\n",
        "        \"info\": {\"description\": \"Combined COCO dataset\"},\n",
        "        \"licenses\": [],\n",
        "        \"images\": combined_images,\n",
        "        \"annotations\": combined_annotations,\n",
        "        \"categories\": combined_categories\n",
        "    }\n",
        "\n",
        "    with open(output_ann_path, \"w\") as f:\n",
        "        json.dump(combined_data, f, indent=4)\n",
        "\n",
        "    print(\"\\nVeri birleÅŸtirme tamamlandÄ±.\")\n",
        "\n",
        "# --- BirleÅŸtirme Ä°ÅŸlemini BaÅŸlat ---\n",
        "paths_to_combine = [\n",
        "    (annotations_path_1, source_dir_1),\n",
        "    (annotations_path_2, source_dir_2)\n",
        "]\n",
        "combine_coco_datasets(paths_to_combine, combined_annotations_path, combined_dataset_dir)\n",
        "\n",
        "\n",
        "# === 3. Analiz iÃ§in SÄ±nÄ±f Ä°simleri ve Metadata ===\n",
        "class_names = [\n",
        "    \"defect\", \"1- patlak\", \"2- igne_kirigi\", \"3- jut\", \"5- likra_kacigi\", \"6- yag_lekesi\", \"8- May cizgisi\", \"fsa\"\n",
        "]\n",
        "\n",
        "unique_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n",
        "dataset_name = f\"tekstil_combined_dataset_{unique_suffix}\"\n",
        "\n",
        "try:\n",
        "    register_coco_instances(dataset_name, {}, combined_annotations_path, combined_dataset_dir)\n",
        "except ValueError:\n",
        "    pass\n",
        "\n",
        "MetadataCatalog.get(dataset_name).thing_classes = class_names\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# === 4. Model YÃ¼kleme ===\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "cfg.MODEL.WEIGHTS = weights_path\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(class_names)\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "print(\"\\nModel loaded with Detectron2 DefaultPredictor.\")\n",
        "\n",
        "\n",
        "# === 5. Tahmin ve KarÅŸÄ±laÅŸtÄ±rma ===\n",
        "gt_map = {}\n",
        "filename_to_id = {}\n",
        "all_image_paths = []\n",
        "\n",
        "with open(combined_annotations_path, \"r\") as f:\n",
        "    coco_ann = json.load(f)\n",
        "    for img in coco_ann[\"images\"]:\n",
        "        filename = img[\"file_name\"]\n",
        "        img_id = img[\"id\"]\n",
        "        full_path = os.path.join(combined_dataset_dir, filename)\n",
        "        filename_to_id[filename] = img_id\n",
        "        all_image_paths.append(full_path)\n",
        "    for ann in coco_ann[\"annotations\"]:\n",
        "        gt_map.setdefault(ann[\"image_id\"], []).append(ann[\"category_id\"])\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "print(f\"\\nAnalyzing {len(all_image_paths)} images...\")\n",
        "for image_path in tqdm(all_image_paths):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    pred_classes = instances.pred_classes.numpy().tolist()\n",
        "\n",
        "    image_id = filename_to_id.get(os.path.basename(image_path))\n",
        "    gt_classes = gt_map.get(image_id, [])\n",
        "\n",
        "    for i in range(len(gt_classes)):\n",
        "        y_true.append(gt_classes[i])\n",
        "        if i < len(pred_classes):\n",
        "            y_pred.append(pred_classes[i] + 1)\n",
        "        else:\n",
        "            y_pred.append(-1)\n",
        "\n",
        "    for i in range(len(gt_classes), len(pred_classes)):\n",
        "        y_true.append(-2)\n",
        "        y_pred.append(pred_classes[i] + 1)\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "    save_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "# --- 6. Genel BaÅŸarÄ± Matrisi (TP/FP/FN/TN) OluÅŸturma ---\n",
        "\n",
        "# TÃ¼m defect sÄ±nÄ±flarÄ±nÄ± \"HatalÄ±\" olarak, None ve DiÄŸer sÄ±nÄ±flarÄ± \"HatasÄ±z\" olarak kabul etme\n",
        "defect_class_ids = set(range(1, len(class_names) + 1))\n",
        "y_true_binary = [1 if val in defect_class_ids else 0 for val in y_true]\n",
        "y_pred_binary = [1 if val in defect_class_ids else 0 for val in y_pred]\n",
        "\n",
        "# 2x2 karÄ±ÅŸÄ±klÄ±k matrisini hesapla\n",
        "cm = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "total_samples = tn + fp + fn + tp\n",
        "\n",
        "# Matrisi istenen dÃ¼zende gÃ¶rselleÅŸtirme\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.set_title('Genel BaÅŸarÄ± Matrisi', fontsize=18, pad=20)\n",
        "ax.set_xlabel('Tahmin Edilen', fontsize=14)\n",
        "ax.set_ylabel('GerÃ§ek', fontsize=14)\n",
        "\n",
        "# Renk haritasÄ± iÃ§in veri\n",
        "heatmap_data = np.array([[tp, fp], [fn, tn]])\n",
        "normalized_data = heatmap_data / np.max(heatmap_data) if np.max(heatmap_data) > 0 else np.zeros((2,2))\n",
        "\n",
        "# Tek bir mavi renk tonu kullanma\n",
        "cmap = plt.cm.get_cmap('Blues')\n",
        "ax.imshow(normalized_data, cmap=cmap, interpolation='nearest', origin='upper')\n",
        "\n",
        "# Metinleri hÃ¼crelere yazma\n",
        "labels_text = [\n",
        "    [f\"TP\\n{tp}\\n({tp/total_samples*100:.2f}%)\", f\"FP\\n{fp}\\n({fp/total_samples*100:.2f}%)\"],\n",
        "    [f\"FN\\n{fn}\\n({fn/total_samples*100:.2f}%)\", f\"TN\\n{tn}\\n({tn/total_samples*100:.2f}%)\"]\n",
        "]\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        color = 'white' if normalized_data[i, j] > 0.5 else 'black'\n",
        "        ax.text(j, i, labels_text[i][j], ha='center', va='center', color=color, fontsize=16, weight='bold')\n",
        "\n",
        "# Eksen etiketlerini ve iÅŸaretlerini ayarla\n",
        "ax.set_xticks(np.arange(2))\n",
        "ax.set_yticks(np.arange(2))\n",
        "ax.set_xticklabels([\"HatalÄ±\", \"HatasÄ±z\"])\n",
        "ax.set_yticklabels([\"HatalÄ±\", \"HatasÄ±z\"])\n",
        "\n",
        "plt.tight_layout()\n",
        "matrix_path = os.path.join(output_dir, \"genel_basari_matrisi.png\")\n",
        "plt.savefig(matrix_path, bbox_inches='tight', pad_inches=0.5)\n",
        "plt.close()\n",
        "\n",
        "# SÄ±nÄ±flandÄ±rma raporunu (precision, recall, f1-score) kaydet\n",
        "report = classification_report(\n",
        "    y_true_binary, y_pred_binary, labels=[0, 1],\n",
        "    target_names=[\"HatasÄ±z\", \"HatalÄ±\"],\n",
        "    zero_division=0\n",
        ")\n",
        "report_path = os.path.join(output_dir, \"binary_classification_report.txt\")\n",
        "with open(report_path, \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(f\"\\nAnaliz tamamlandÄ±. Genel BaÅŸarÄ± Matrisi buraya kaydedildi: {output_dir}\")"
      ],
      "metadata": {
        "id": "lvZedlnwjTcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama (Colab iÃ§in)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === Dosya YollarÄ± ===\n",
        "# TaÅŸÄ±nacak kaynak dosya (yeni konumu)\n",
        "source_file = \"/#your file path/ikinci.coco.json\"\n",
        "\n",
        "# DosyanÄ±n eski konumu (hedef klasÃ¶r)\n",
        "destination_dir = \"/#your file path/train\"\n",
        "destination_path = os.path.join(destination_dir, \"ikinci.coco.json\")\n",
        "\n",
        "# === DosyayÄ± TaÅŸÄ±ma ===\n",
        "if os.path.exists(source_file):\n",
        "    try:\n",
        "        shutil.move(source_file, destination_path)\n",
        "        print(f\"'{source_file}' dosyasÄ± baÅŸarÄ±yla '{destination_dir}' klasÃ¶rÃ¼ne geri taÅŸÄ±ndÄ±.\")\n",
        "    except Exception as e:\n",
        "        print(f\"DosyayÄ± taÅŸÄ±rken bir hata oluÅŸtu: {e}\")\n",
        "else:\n",
        "    print(f\"Hata: '{source_file}' dosyasÄ± belirtilen yolda bulunamadÄ±.\")"
      ],
      "metadata": {
        "id": "g9kt54qUW3pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama (Colab iÃ§in)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# === Dosya YollarÄ± ===\n",
        "# TaÅŸÄ±nacak kaynak dosyalar\n",
        "source_file_1 = \"/#your file path/ikinci.coco.json\"\n",
        "source_file_2 = \"/#your file path/train/ilk.coco.json\"\n",
        "\n",
        "# DosyalarÄ±n taÅŸÄ±nacaÄŸÄ± hedef klasÃ¶r.\n",
        "destination_dir = \"/content/drive/MyDrive/jsonss\"\n",
        "\n",
        "# === 1. Hedef KlasÃ¶rÃ¼ OluÅŸturma ===\n",
        "# EÄŸer 'jsonss' klasÃ¶rÃ¼ yoksa, oluÅŸtur.\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "print(f\"Hedef klasÃ¶r hazÄ±r: {destination_dir}\\n\")\n",
        "\n",
        "# === 2. DosyalarÄ± TaÅŸÄ±ma ===\n",
        "files_to_move = [source_file_1, source_file_2]\n",
        "\n",
        "for file_path in files_to_move:\n",
        "    # Dosya adÄ±nÄ± al\n",
        "    file_name = os.path.basename(file_path)\n",
        "\n",
        "    # Yeni dosya yolunu oluÅŸtur\n",
        "    destination_path = os.path.join(destination_dir, file_name)\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            # DosyayÄ± taÅŸÄ±\n",
        "            shutil.move(file_path, destination_path)\n",
        "            print(f\"'{file_name}' dosyasÄ± baÅŸarÄ±yla taÅŸÄ±ndÄ±.\")\n",
        "        except Exception as e:\n",
        "            print(f\"'{file_name}' dosyasÄ±nÄ± taÅŸÄ±rken bir hata oluÅŸtu: {e}\")\n",
        "    else:\n",
        "        print(f\"Hata: '{file_name}' dosyasÄ± kaynak yolda bulunamadÄ±.\")\n",
        "\n",
        "print(\"\\nTaÅŸÄ±ma iÅŸlemi tamamlandÄ±.\")"
      ],
      "metadata": {
        "id": "SUbw68_sWIg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "ann_path = \"/#your file path3/train/_annotations.coco.json\"\n",
        "print(\"Dosya var mÄ±?\", os.path.exists(ann_path))\n"
      ],
      "metadata": {
        "id": "tKxbHZ31By72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2 import _C   # detectron2 nin yol uyumsuzlugu Ã§Ä±kÄ±nca kullandÄ±ÄŸÄ±m kod\n",
        "\n",
        "print(_C.__file__)\n"
      ],
      "metadata": {
        "id": "4-ajPzHj-rAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarma\n",
        "import torch\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "import os\n",
        "import json\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Logger'Ä± ayarlama\n",
        "setup_logger()\n",
        "\n",
        "# 1. EÄŸitilmiÅŸ Modeli ve KonfigÃ¼rasyon DosyalarÄ±nÄ± YÃ¼kleme\n",
        "print(\"EÄŸitilmiÅŸ model dosyalarÄ± yÃ¼kleniyor...\")\n",
        "\n",
        "# Model ve konfigÃ¼rasyon dosyalarÄ±nÄ±n yollarÄ±\n",
        "model_dir = \"/#your file path/model1\"\n",
        "model_path = os.path.join(model_dir, \"model_final.pth\")\n",
        "config_path = os.path.join(model_dir, \"config.yml\")\n",
        "\n",
        "# KonfigÃ¼rasyon dosyasÄ±nÄ± yÃ¼kle\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(config_path)\n",
        "\n",
        "# EÄŸitilmiÅŸ modelin aÄŸÄ±rlÄ±klarÄ±nÄ± yÃ¼kle\n",
        "cfg.MODEL.WEIGHTS = model_path\n",
        "\n",
        "# DÃ¼ÅŸÃ¼k gÃ¼venilirlikteki tahminleri filtrelemek iÃ§in eÅŸik deÄŸerini dÃ¼ÅŸÃ¼rdÃ¼k.\n",
        "# Bu, modelin daha fazla nesne bulmasÄ±nÄ± saÄŸlayacaktÄ±r.\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3  # Hassasiyet dÃ¼ÅŸÃ¼rÃ¼ldÃ¼.\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# EÄŸitilmiÅŸ model ile tahmin yapmak iÃ§in bir Ã¶ngÃ¶rÃ¼cÃ¼ (predictor) oluÅŸtur\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# 2. Resimleri Analiz Etme ve GÃ¶rselleÅŸtirme\n",
        "print(\"Foto-Arsivi/lab_data2_gray klasÃ¶rÃ¼ndeki fotoÄŸraflar analiz ediliyor...\")\n",
        "\n",
        "# Analiz edilecek resimlerin bulunduÄŸu klasÃ¶rÃ¼n yeni yolu\n",
        "test_image_dir = \"/#your file path/lab_data2_gray\"\n",
        "\n",
        "# JPG ve PNG uzantÄ±lÄ± tÃ¼m dosyalarÄ± listele\n",
        "image_files = [f for f in os.listdir(test_image_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
        "\n",
        "if not image_files:\n",
        "    print(\"Hata: Belirtilen klasÃ¶rde hiÃ§ JPG veya PNG dosyasÄ± bulunamadÄ±. LÃ¼tfen yolu ve dosya uzantÄ±sÄ±nÄ± kontrol edin.\")\n",
        "else:\n",
        "    # Her bir resim dosyasÄ± iÃ§in tahmin yap\n",
        "    for filename in image_files:\n",
        "        image_path = os.path.join(test_image_dir, filename)\n",
        "\n",
        "        img = cv2.imread(image_path)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Hata: {filename} dosyasÄ± okunamadÄ±. Atlama yapÄ±lÄ±yor.\")\n",
        "            continue\n",
        "\n",
        "        outputs = predictor(img)\n",
        "\n",
        "        # GÃ¶rselleÅŸtirme iÃ§in Visualizer oluÅŸtur\n",
        "        v = Visualizer(img[:, :, ::-1], scale=1.0)\n",
        "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "        print(f\"Analiz edilen dosya: {filename}\")\n",
        "        cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "\n",
        "print(\"\\nNesne tespiti ve gÃ¶rselleÅŸtirme tamamlandÄ±.\")\n"
      ],
      "metadata": {
        "id": "fn1PTMuT05o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Google Drive'Ä± baÄŸla\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. YollarÄ± ayarla\n",
        "model_dir = \"/#your file path/model3\"\n",
        "test_dir = \"/#your file path/test\"\n",
        "test_json = os.path.join(test_dir, \"_annotations.coco.json\")\n",
        "gray_dir = \"/#your file path/lab_data2_gray\"  # Ã–rnek etiket yok klasÃ¶r\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/Detectron2_Results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "test_vis_dir = os.path.join(output_dir, \"test_visualizations\")\n",
        "gray_vis_dir = os.path.join(output_dir, \"gray_visualizations\")\n",
        "os.makedirs(test_vis_dir, exist_ok=True)\n",
        "os.makedirs(gray_vis_dir, exist_ok=True)\n",
        "\n",
        "# 3. Test datasÄ±nÄ± COCO olarak kayÄ±t et\n",
        "dataset_name = \"tekstil_test_v2\"\n",
        "register_coco_instances(dataset_name, {}, test_json, test_dir)\n",
        "\n",
        "# 4. COCO JSON'dan sÄ±nÄ±f isimlerini Ã§ek ve metadata'ya ver\n",
        "with open(test_json, \"r\") as f:\n",
        "    coco_data = json.load(f)\n",
        "thing_classes = [cat[\"name\"] for cat in coco_data[\"categories\"]]\n",
        "MetadataCatalog.get(dataset_name).thing_classes = thing_classes\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "# 5. Detectron2 modeli yÃ¼kle ve ayarla\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(os.path.join(model_dir, \"config.yml\"))\n",
        "cfg.MODEL.WEIGHTS = os.path.join(model_dir, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2  # Hassasiyet artÄ±rÄ±ldÄ± (eÅŸik dÃ¼ÅŸÃ¼rÃ¼ldÃ¼)\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# 6. Test gÃ¶rÃ¼ntÃ¼lerinde tahmin yap, gÃ¶rselleÅŸtir ve confusion matrix iÃ§in gerÃ§ek ve tahminleri topla\n",
        "dataset_dicts = DatasetCatalog.get(dataset_name)\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "print(\"Test gÃ¶rÃ¼ntÃ¼leri Ã¼zerinde tahmin ve confusion matrix hesaplanÄ±yor...\")\n",
        "for data in dataset_dicts:\n",
        "    img_path = data[\"file_name\"]\n",
        "    img = cv2.imread(img_path)\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "    # GerÃ§ek sÄ±nÄ±flar (etiketler)\n",
        "    gt_classes = [ann[\"category_id\"] for ann in data[\"annotations\"]]\n",
        "    # Tahmin edilen sÄ±nÄ±flar\n",
        "    pred_classes = instances.pred_classes.tolist()\n",
        "\n",
        "    y_true.extend(gt_classes)\n",
        "    y_pred.extend(pred_classes)\n",
        "\n",
        "    # GÃ¶rselleÅŸtir ve kaydet\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    save_path = os.path.join(test_vis_dir, os.path.basename(img_path))\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "print(\"Test gÃ¶rselleri tahminleri ve gÃ¶rselleri kaydedildi.\")\n",
        "\n",
        "# Confusion matrix Ã§iz\n",
        "labels = list(range(len(thing_classes)))\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=thing_classes)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "disp.plot(xticks_rotation=45, cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix - Test Set\")\n",
        "plt.tight_layout()\n",
        "cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
        "plt.savefig(cm_path)\n",
        "plt.show()\n",
        "print(f\"Confusion matrix gÃ¶rseli kaydedildi: {cm_path}\")\n",
        "\n",
        "# 7. Etiketsiz gray klasÃ¶rÃ¼ndeki gÃ¶rÃ¼ntÃ¼ler iÃ§in sadece tahmin yap ve gÃ¶rsel kaydet\n",
        "print(\"\\nEtiketsiz (gray) klasÃ¶rÃ¼ndeki gÃ¶rÃ¼ntÃ¼ler tahmin ediliyor ve kaydediliyor...\")\n",
        "\n",
        "gray_images = [f for f in os.listdir(gray_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
        "\n",
        "for filename in gray_images:\n",
        "    img_path = os.path.join(gray_dir, filename)\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"UyarÄ±: {filename} okunamadÄ±, atlanÄ±yor.\")\n",
        "        continue\n",
        "\n",
        "    outputs = predictor(img)\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "    out = v.draw_instance_predictions(instances)\n",
        "    out_img = out.get_image()[:, :, ::-1]\n",
        "\n",
        "    save_path = os.path.join(gray_vis_dir, filename)\n",
        "    cv2.imwrite(save_path, out_img)\n",
        "\n",
        "print(\"Gray klasÃ¶rÃ¼ndeki gÃ¶rseller iÅŸlenip kaydedildi.\")\n",
        "print(\"\\nTÃ¼m iÅŸlem tamamlandÄ±.\")\n"
      ],
      "metadata": {
        "id": "5Z-GdRRH7_4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# JSON dosya yolu\n",
        "json_path = \"/#your file path/test/_annotations.coco.json\"\n",
        "\n",
        "# JSON dosyasÄ±nÄ± aÃ§ ve iÃ§eriÄŸini yÃ¼kle\n",
        "with open(json_path, \"r\") as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "# \"categories\" listesinden sÄ±nÄ±f isimlerini Ã§Ä±kar\n",
        "thing_classes = [cat[\"name\"] for cat in coco_data[\"categories\"]]\n",
        "\n",
        "# SonuÃ§larÄ± yazdÄ±r\n",
        "print(\"SÄ±nÄ±f isimleri (thing_classes):\", thing_classes)\n"
      ],
      "metadata": {
        "id": "Hwmx97A18WtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJ4NPFHwYv7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarma\n",
        "import cv2\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Drive'Ä± baÄŸlama\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. KlasÃ¶r yollarÄ±nÄ± ve Ã§Ä±ktÄ± ayarlarÄ±nÄ± belirleme\n",
        "# LÃœTFEN BU YOLLARIN VE AYARLARIN DOÄžRU OLDUÄžUNDAN EMÄ°N OLUN!\n",
        "\n",
        "# Analiz edilecek resimlerin bulunduÄŸu kaynak klasÃ¶r\n",
        "source_dir = \"/#your file path/lab_data2_HD\"\n",
        "\n",
        "# Ä°ÅŸlenmiÅŸ resimlerin kaydedileceÄŸi hedef klasÃ¶r\n",
        "# Bu klasÃ¶r otomatik olarak oluÅŸturulacaktÄ±r.\n",
        "destination_dir = \"/#your file path/lab_data2_gray\"\n",
        "\n",
        "# Resimlerin kÃ¼Ã§Ã¼ltÃ¼leceÄŸi hedef boyut (geniÅŸlik, yÃ¼kseklik)\n",
        "# Ã–rnek: (640, 480) veya (1280, 720) gibi bir deÄŸer girebilirsiniz.\n",
        "target_size = (640, 480)\n",
        "\n",
        "# 2. KlasÃ¶rlerin varlÄ±ÄŸÄ±nÄ± kontrol etme ve oluÅŸturma\n",
        "\n",
        "# Kaynak klasÃ¶rÃ¼n varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "if not os.path.exists(source_dir):\n",
        "    print(f\"Hata: Kaynak klasÃ¶r bulunamadÄ±: {source_dir}\")\n",
        "    exit()\n",
        "\n",
        "# Hedef klasÃ¶rÃ¼ oluÅŸtur\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "print(f\"Hedef klasÃ¶r oluÅŸturuldu veya zaten mevcut: {destination_dir}\\n\")\n",
        "\n",
        "# 3. Resimleri iÅŸleme ve kaydetme\n",
        "\n",
        "print(\"Resimler iÅŸlenmeye baÅŸlÄ±yor...\")\n",
        "\n",
        "# Kaynak klasÃ¶rdeki tÃ¼m dosyalarÄ± dÃ¶ngÃ¼ye al\n",
        "for filename in os.listdir(source_dir):\n",
        "    # DosyanÄ±n PNG formatÄ±nda olup olmadÄ±ÄŸÄ±nÄ± kontrol et\n",
        "    if filename.lower().endswith('.png'):\n",
        "        source_path = os.path.join(source_dir, filename)\n",
        "\n",
        "        # Resmi oku (OpenCV, resimleri varsayÄ±lan olarak BGR formatÄ±nda okur)\n",
        "        img = cv2.imread(source_path)\n",
        "\n",
        "        if img is None:\n",
        "            print(f\"Hata: {filename} dosyasÄ± okunamadÄ±. Atlama yapÄ±lÄ±yor.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Resim boyutunu kÃ¼Ã§Ã¼ltme\n",
        "            resized_img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # Yeni dosya adÄ± ve yolu oluÅŸturma\n",
        "            # .png uzantÄ±sÄ±nÄ± .jpg ile deÄŸiÅŸtir\n",
        "            new_filename = os.path.splitext(filename)[0] + '.jpg'\n",
        "            destination_path = os.path.join(destination_dir, new_filename)\n",
        "\n",
        "            # Boyutu kÃ¼Ã§Ã¼ltÃ¼lmÃ¼ÅŸ renkli resmi JPG olarak kaydetme\n",
        "            # ArtÄ±k gri tonlamaya dÃ¶nÃ¼ÅŸtÃ¼rme iÅŸlemi yapÄ±lmÄ±yor.\n",
        "            cv2.imwrite(destination_path, resized_img)\n",
        "\n",
        "            print(f\"'{filename}' baÅŸarÄ±yla iÅŸlendi ve '{new_filename}' olarak kaydedildi.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Hata: '{filename}' dosyasÄ± iÅŸlenirken bir sorun oluÅŸtu. Hata: {e}\")\n",
        "\n",
        "print(\"\\nTÃ¼m resimler baÅŸarÄ±yla iÅŸlendi ve kaydedildi.\")\n"
      ],
      "metadata": {
        "id": "rWHPdjfh0Sx5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}